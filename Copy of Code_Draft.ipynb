{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11119,"status":"ok","timestamp":1732774440407,"user":{"displayName":"Moriwam","userId":"13104889693492672636"},"user_tz":-360},"id":"jg04QNR42nXF","outputId":"24b9ae44-76d6-433f-ef42-73e8b018263f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (1.1.2)\n","Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (5.3.0)\n","Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n","Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.4)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.26.4)\n","Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.13.1)\n","Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.5.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (7.34.0)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython) (75.1.0)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython) (0.19.2)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython) (0.7.5)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython) (5.7.1)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython) (3.0.48)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython) (2.18.0)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython) (0.1.7)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython) (4.9.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython) (0.8.4)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython) (0.2.13)\n"]}],"source":["!pip3 install python-docx\n","!pip3 install imbalanced-learn\n","!pip3 install ipython"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3647,"status":"ok","timestamp":1732774446111,"user":{"displayName":"Moriwam","userId":"13104889693492672636"},"user_tz":-360},"id":"Q9MQOy2qPiTa","outputId":"87c061f9-52b8-4ab9-df21-b6dae6d6511f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12794,"status":"ok","timestamp":1732774461751,"user":{"displayName":"Moriwam","userId":"13104889693492672636"},"user_tz":-360},"id":"naIFbHot5aF2","outputId":"bb429ad5-8673-4af9-894e-8279ca7e8383"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}],"source":["import os\n","import warnings\n","import random\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder, StandardScaler, label_binarize\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, confusion_matrix, classification_report, roc_curve, ConfusionMatrixDisplay,precision_recall_fscore_support\n","# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","from google.colab import files, drive\n","from docx import Document\n","from docx.shared import Inches\n","from io import StringIO\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from IPython.display import display, HTML\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.utils import to_categorical\n","\n","# Set environment variables for reproducibility\n","os.environ['PYTHONHASHSEED'] = '42'\n","os.environ['TF_DETERMINISTIC_OPS'] = '1'\n","\n","# Set random seeds for reproducibility across all libraries\n","random.seed(42)\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","# Suppress all warnings\n","warnings.filterwarnings('ignore')\n","\n","\n","drive.mount('/content/drive/')\n"]},{"cell_type":"markdown","metadata":{"id":"OUS3AUmSEBbT"},"source":["## Make directory"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":443,"status":"ok","timestamp":1732774472449,"user":{"displayName":"Moriwam","userId":"13104889693492672636"},"user_tz":-360},"id":"zEJzCyxX5aZt"},"outputs":[],"source":["class DataPipeline:\n","    def __init__(self, file_path, base_output_file):\n","        self.file_path = file_path\n","        self.base_output_file = base_output_file\n","        self.df = None\n","        self.X = None\n","        self.y = None\n","        self.num_classes = None\n","\n","        # Create the results directory if it doesn't exist\n","        self.results_dir = base_output_file\n","        self.dataset_results_dir = os.path.join(self.results_dir, 'dataset')\n","        self._ensure_directories_exist()\n","\n","    def _ensure_directories_exist(self):\n","        \"\"\"Ensure the results and dataset directories exist.\"\"\"\n","        if not os.path.exists(self.results_dir):\n","            os.makedirs(self.results_dir)\n","            print(f\"Created directory: {self.results_dir}\")\n","\n","        if not os.path.exists(self.dataset_results_dir):\n","            os.makedirs(self.dataset_results_dir)\n","            print(f\"Created directory: {self.dataset_results_dir}\")\n","\n","    def load_dataset(self):\n","        self.df = pd.read_csv(self.file_path)\n","        return self.df\n","\n","    def plot_class_distribution(self, y, plot_title='Class Distribution'):\n","        plt.figure(figsize=(6, 4))\n","        y.value_counts().plot(kind='bar', color='green', alpha=0.7)\n","        plt.title(plot_title)\n","        plt.xlabel('Classes')\n","        plt.ylabel('Frequency')\n","        plot_path = os.path.join(self.dataset_results_dir, f'{plot_title}.png')\n","        plt.savefig(plot_path)\n","        plt.close()\n","\n","    def display_dataset_summary(self):\n","        dataset_output_file_path = os.path.join(self.dataset_results_dir, 'dataset_summary.csv')\n","        with open(dataset_output_file_path, 'w') as f:\n","            f.write('--- Head of Dataset ---\\n')\n","            self.df.head().to_csv(f, index=False)\n","            f.write('\\n')\n","\n","            f.write('--- Describe of Dataset ---\\n')\n","            describe_df = self.df.describe().transpose()\n","            describe_df.to_csv(f)\n","            f.write('\\n')\n","\n","            f.write('--- Info of Dataset ---\\n')\n","            buffer = StringIO()\n","            self.df.info(buf=buffer)\n","            info_lines = buffer.getvalue().splitlines()\n","\n","            for line in info_lines:\n","                if 'Non-Null Count' in line:\n","                    f.write('Column, Non-Null Count, Dtype\\n')\n","                elif '<class' not in line and 'memory' not in line:\n","                    parts = line.split()\n","                    if len(parts) > 3:\n","                        column_name = parts[0]\n","                        non_null_count = parts[-2]\n","                        dtype = parts[-1]\n","                        f.write(f'{column_name}, {non_null_count}, {dtype}\\n')\n","\n","        print(f\"Dataset summary saved to {dataset_output_file_path}\")\n","\n","    def data_distribution_plot(self, df):\n","        dataset_images_directory = os.path.join(self.dataset_results_dir, 'images')\n","\n","        # Ensure the directory exists before saving plots\n","        os.makedirs(dataset_images_directory, exist_ok=True)\n","\n","        numerical_cols = df.select_dtypes(include=[np.number]).columns\n","\n","        # Plot distribution for numerical columns\n","        for col in numerical_cols:\n","            plt.figure(figsize=(6, 4))\n","            sns.histplot(df[col], kde=True, bins=30)\n","            plt.title(f'Distribution of {col}')\n","            plt.xlabel(col)\n","            plt.ylabel('Frequency')\n","            plot_path = os.path.join(dataset_images_directory, f'Distribution_{col}.png')\n","            plt.savefig(plot_path)\n","            plt.close()  # Close the plot to avoid overlap\n","\n","        categorical_cols = df.select_dtypes(include=['object']).columns\n","\n","        # Plot distribution for categorical columns\n","        for col in categorical_cols:\n","            plt.figure(figsize=(6, 4))\n","            sns.countplot(data=df, x=col)\n","            plt.title(f'Distribution of {col}')\n","            plt.xlabel(col)\n","            plt.ylabel('Frequency')\n","            plt.xticks(rotation=45)\n","            plot_path = os.path.join(dataset_images_directory, f'Distribution_{col}.png')\n","            plt.savefig(plot_path)\n","            plt.close()  # Close the plot to avoid overlap\n","\n","    def clean_data(self):\n","        cleaned_file_path = os.path.join(self.dataset_results_dir, 'cleaned_dataset.csv')\n","        numeric_cols = self.df.select_dtypes(include=['number']).columns\n","        rounded_means = self.df[numeric_cols].mean().round()\n","        self.df[numeric_cols] = self.df[numeric_cols].fillna(rounded_means)\n","\n","        categorical_cols = self.df.select_dtypes(include=['object']).columns\n","        for col in categorical_cols:\n","            self.df[col] = self.df[col].fillna(self.df[col].mode()[0])\n","\n","        self.df.to_csv(cleaned_file_path, index=False)\n","        print(f\"Cleaned dataset saved to {cleaned_file_path}\")\n","        return self.df\n","\n","    def define_target(self, df_cleaned, target_column_name):\n","        self.y = df_cleaned[target_column_name]\n","        self.X = df_cleaned.drop(columns=[target_column_name])\n","        self.num_classes = len(np.unique(self.y))\n","        print(f\"Detected {self.num_classes} unique classes in the target column.\")\n","        return self.X, self.y\n","\n","    def encode_categorical(self):\n","        label_encoders = {}\n","\n","        # Check for categorical columns in X and apply LabelEncoder only if categorical columns exist\n","        categorical_columns = self.X.select_dtypes(include=['object']).columns\n","\n","        # If categorical columns exist, apply LabelEncoder\n","        if not categorical_columns.empty:\n","            for col in categorical_columns:\n","                label_encoders[col] = LabelEncoder()\n","                self.X[col] = label_encoders[col].fit_transform(self.X[col])\n","\n","        # Encode the target variable (y) if necessary\n","        if self.y.dtype == 'object' or self.y.dtype.name == 'category':\n","            y_encoder = LabelEncoder()\n","            y_encoded = y_encoder.fit_transform(self.y)\n","        else:\n","            y_encoded = self.y  # If y is numeric, no encoding is needed\n","\n","        # Return the updated X and y_encoded\n","        return self.X, y_encoded\n","\n","    def split_data(self, test_size, random_state=42):\n","        X_encoded, y_encoded = self.encode_categorical()\n","        X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=test_size, random_state=random_state, stratify=self.y)\n","        return X_train, X_test, y_train, y_test\n","\n","    def apply_standard_scaler(self, X_train, X_test):\n","        scaler = StandardScaler()\n","        X_train_scaled = scaler.fit_transform(X_train)\n","        X_test_scaled = scaler.transform(X_test)\n","        return X_train_scaled, X_test_scaled\n","\n","    def tune_model_params(self, default_model):\n","        param_defaults = {\n","        \"RandomForestClassifier\": {\n","            \"n_estimators\": 100,  # Number of trees in the forest. Other possible values: any positive integer, default=100\n","            \"max_depth\": 15,  # Maximum depth of the tree. Other possible values: any positive integer, default=None\n","            \"min_samples_split\": 5,  # Minimum number of samples required to split an internal node. Other possible values: any positive integer, default=2\n","            \"min_samples_leaf\": 2,  # Minimum number of samples required to be at a leaf node. Other possible values: any positive integer, default=1\n","            \"criterion\": 'gini',  # Function to measure the quality of a split. Other possible values: 'entropy', 'log_loss', default='gini'\n","            \"random_state\": 42  # Random seed for reproducibility. Other possible values: any integer, default=None\n","        },\n","        \"SVC\": {\n","            \"C\": 1.0,  # Regularization parameter. Other possible values: any positive float, default=1.0\n","            \"kernel\": 'rbf',  # Kernel type. Other possible values: 'linear', 'poly', 'sigmoid', 'precomputed', default='rbf'\n","            \"gamma\": 'scale',  # Kernel coefficient. Other possible values: 'auto', any positive float, default='scale'\n","            \"degree\": 3,  # Degree of the polynomial kernel function ('poly'). Other possible values: any positive integer, default=3\n","            \"random_state\": 42  # Random seed for reproducibility. Other possible values: any integer, default=None\n","        },\n","        \"DecisionTreeClassifier\": {\n","            \"max_depth\": 20,  # Maximum depth of the tree. Other possible values: any positive integer, default=None\n","            \"min_samples_split\": 10,  # Minimum number of samples required to split an internal node. Other possible values: any positive integer, default=2\n","            \"min_samples_leaf\": 4,  # Minimum number of samples required to be at a leaf node. Other possible values: any positive integer, default=1\n","            \"criterion\": 'entropy',  # Function to measure the quality of a split. Other possible values: 'gini', default='gini'\n","            \"random_state\": 42  # Random seed for reproducibility. Other possible values: any integer, default=None\n","        },\n","        \"KNeighborsClassifier\": {\n","            \"n_neighbors\": 7,  # Number of neighbors to use. Other possible values: any positive integer, default=5\n","            \"weights\": 'distance',  # Weight function used in prediction. Other possible values: 'uniform', default='uniform'\n","            \"algorithm\": 'auto',  # Algorithm used to compute the nearest neighbors. Other possible values: 'ball_tree', 'kd_tree', 'brute', default='auto'\n","            \"leaf_size\": 30  # Leaf size passed to BallTree or KDTree. Other possible values: any positive integer, default=30\n","        },\n","        \"LogisticRegression\": {\n","            \"C\": 0.5,  # Inverse of regularization strength. Other possible values: any positive float, default=1.0\n","            \"solver\": 'liblinear',  # Algorithm to use in the optimization problem. Other possible values: 'newton-cg', 'lbfgs', 'sag', 'saga', default='lbfgs'\n","            \"max_iter\": 200,  # Maximum number of iterations. Other possible values: any positive integer, default=100\n","            \"penalty\": 'l2',  # Norm used in the penalization. Other possible values: 'l1', 'elasticnet', None, default='l2'\n","            \"random_state\": 42  # Random seed for reproducibility. Other possible values: any integer, default=None\n","        }\n","    }\n","\n","        model_name = default_model.__class__.__name__\n","        if model_name in param_defaults:\n","            params = param_defaults[model_name]\n","            return default_model.set_params(**params)\n","        else:\n","            print(f\"\\nNo specific tuning parameters available for {model_name}. Using default parameters.\")\n","            return default_model\n","\n","    def plot_metrics(self, metrics_summary, output_directory, doc):\n","        models = metrics_summary['Model']\n","        metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n","\n","        for metric in metrics:\n","            plt.figure()\n","            plt.bar(models, metrics_summary[metric], color='skyblue')\n","            plt.xlabel('Model')\n","            plt.ylabel(metric)\n","            plt.title(f'Model Comparison: {metric}')\n","            metric_plot_path = os.path.join(output_directory, f'{metric}_comparison.png')\n","            plt.savefig(metric_plot_path)\n","            plt.close()\n","\n","            # Add the plot to the document\n","            doc.add_heading(f'{metric} Comparison', level=2)\n","            doc.add_picture(metric_plot_path, width=Inches(4.5))\n","            doc.add_paragraph(\"-\" * 50)\n","\n","    def train_and_evaluate_models(self, X_train_scaled, X_test_scaled, y_train, y_test, doc, output_directory, num_classes):\n","       # Here you can add more algorithms\n","        models = {\n","            \"Random Forest\": self.tune_model_params(RandomForestClassifier()),\n","            \"SVM\": self.tune_model_params(SVC(probability=True)),\n","            \"Decision Tree\": self.tune_model_params(DecisionTreeClassifier()),\n","            \"KNN\": self.tune_model_params(KNeighborsClassifier()),\n","            \"Logistic Regression\": self.tune_model_params(LogisticRegression())\n","        }\n","\n","        metrics_summary = {\n","            'Model': [],\n","            'Accuracy': [],\n","            'Precision': [],\n","            'Recall': [],\n","            'F1-Score': [],\n","            'ROC AUC': []\n","        }\n","\n","        for name, model in models.items():\n","            print(f\"\\nTraining and evaluating {name}...\")\n","            model.fit(X_train_scaled, y_train)\n","            y_pred = model.predict(X_test_scaled)\n","            y_prob = model.predict_proba(X_test_scaled) if hasattr(model, \"predict_proba\") else model.decision_function(X_test_scaled)\n","\n","            accuracy = accuracy_score(y_test, y_pred)\n","            precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n","\n","            classification_rep = classification_report(y_test, y_pred)\n","            print(f\"{name} Accuracy: {accuracy:.4f}\")\n","            print(f\"{name} Classification Report:\\n{classification_rep}\\n\")\n","\n","            # Initialize roc_auc as None\n","            roc_auc = None\n","\n","            # Calculate ROC AUC for binary and multi-class cases\n","            if num_classes > 2:\n","                # Multi-class case\n","                y_test_bin = label_binarize(y_test, classes=np.arange(num_classes))\n","                try:\n","                    roc_auc = roc_auc_score(y_test_bin, y_prob, multi_class='ovr')\n","                except ValueError:\n","                    roc_auc = None  # Handle any issues during calculation\n","            else:\n","                # Binary case\n","                try:\n","                    roc_auc = roc_auc_score(y_test, y_prob[:, 1])  # Assuming y_prob is the probability for the positive class\n","                except ValueError:\n","                    roc_auc = None  # Handle any issues during calculation\n","\n","            # Print the ROC AUC result\n","            if roc_auc is not None:\n","                print(f\"{name} ROC AUC: {roc_auc:.4f}\")\n","            else:\n","                print(f\"{name} ROC AUC: Not defined\")\n","\n","\n","            print(\"-\" * 50)\n","\n","            # Save metrics to summary\n","            metrics_summary['Model'].append(name)\n","            metrics_summary['Accuracy'].append(accuracy)\n","            metrics_summary['Precision'].append(precision)\n","            metrics_summary['Recall'].append(recall)\n","            metrics_summary['F1-Score'].append(f1)\n","            metrics_summary['ROC AUC'].append(roc_auc if roc_auc is not None else \"Not defined\")\n","\n","            # Plot ROC curve if ROC AUC is defined\n","            if roc_auc is not None:\n","                if num_classes == 2:\n","                    fpr, tpr, _ = roc_curve(y_test, y_prob[:, 1])\n","                else:\n","                    fpr, tpr, _ = roc_curve(y_test_bin.ravel(), y_prob.ravel())\n","                plt.figure()\n","                plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n","                plt.plot([0, 1], [0, 1], color='grey', linestyle='--')\n","                plt.xlim([0.0, 1.0])\n","                plt.ylim([0.0, 1.05])\n","                plt.xlabel('False Positive Rate')\n","                plt.ylabel('True Positive Rate')\n","                plt.title(f'{name} ROC Curve')\n","                plt.legend(loc=\"lower right\")\n","\n","                # Save the plot\n","                roc_curve_path = os.path.join(output_directory, f'{name}_roc_curve.png')\n","                plt.savefig(roc_curve_path)\n","                plt.close()\n","\n","                # Add results and ROC curve image to the document\n","                doc.add_heading(f'{name} Results', level=2)\n","                doc.add_paragraph(f\"Accuracy: {accuracy:.4f}\")\n","                doc.add_paragraph(\"Classification Report:\")\n","                doc.add_paragraph(classification_rep)\n","                doc.add_paragraph(f\"ROC AUC: {roc_auc:.4f}\")\n","                doc.add_picture(roc_curve_path, width=Inches(4.5))\n","            else:\n","                doc.add_heading(f'{name} Results', level=2)\n","                doc.add_paragraph(f\"Accuracy: {accuracy:.4f}\")\n","                doc.add_paragraph(\"Classification Report:\")\n","                doc.add_paragraph(classification_rep)\n","                doc.add_paragraph(\"ROC AUC: Not defined (only one class present in y_true)\")\n","\n","            doc.add_paragraph(\"-\" * 50)\n","\n","        # -------------------- Add Keras ANN Below -------------------- #\n","\n","        print(\"\\nTraining and evaluating Keras ANN...\")\n","\n","        ann_model = Sequential()\n","        ann_model.add(Dense(units=64, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n","        ann_model.add(Dense(units=32, activation='relu'))\n","\n","        if num_classes > 2:\n","            ann_model.add(Dense(units=num_classes, activation='softmax'))  # Multiclass classification\n","            loss_function = 'sparse_categorical_crossentropy'\n","        else:\n","            ann_model.add(Dense(units=1, activation='sigmoid'))  # Binary classification\n","            loss_function = 'binary_crossentropy'\n","\n","        ann_model.compile(\n","            optimizer='adam',\n","            loss=loss_function,\n","            metrics=['accuracy']\n","        )\n","\n","        ann_model.fit(\n","            X_train_scaled, y_train,\n","            epochs=10,\n","            batch_size=32,\n","            validation_data=(X_test_scaled, y_test),\n","            verbose=1\n","        )\n","\n","        loss, accuracy = ann_model.evaluate(X_test_scaled, y_test, verbose=0)\n","\n","        # Evaluate model based on the number of classes\n","        if len(np.unique(y_test)) == 1:\n","            print(\"Keras ANN ROC AUC: Not defined (only one class present in y_true)\")\n","            roc_auc_ann = None  # ROC AUC cannot be defined\n","            precision_ann = None\n","            recall_ann = None\n","            f1_ann = None\n","        else:\n","            if num_classes > 2:\n","                y_prob_ann = ann_model.predict(X_test_scaled)\n","                y_test_bin = to_categorical(y_test, num_classes=num_classes)\n","                roc_auc_ann = roc_auc_score(y_test_bin, y_prob_ann, multi_class='ovr')\n","                precision_ann = precision_score(y_test_bin.argmax(axis=1), y_prob_ann.argmax(axis=1), average='weighted')\n","                recall_ann = recall_score(y_test_bin.argmax(axis=1), y_prob_ann.argmax(axis=1), average='weighted')\n","                f1_ann = f1_score(y_test_bin.argmax(axis=1), y_prob_ann.argmax(axis=1), average='weighted')\n","            else:\n","                y_prob_ann = ann_model.predict(X_test_scaled).ravel()\n","                y_pred_ann = (y_prob_ann > 0.5).astype(int)  # Convert probabilities to binary predictions\n","                roc_auc_ann = roc_auc_score(y_test, y_prob_ann)\n","                precision_ann = precision_score(y_test, y_pred_ann)\n","                recall_ann = recall_score(y_test, y_pred_ann)\n","                f1_ann = f1_score(y_test, y_pred_ann)\n","\n","        # Append ANN metrics to summary\n","        metrics_summary['Model'].append(\"Keras ANN\")\n","        metrics_summary['Accuracy'].append(float(accuracy))\n","        metrics_summary['Precision'].append(float(precision_ann) if precision_ann is not None else \"Not defined\")\n","        metrics_summary['Recall'].append(float(recall_ann) if recall_ann is not None else \"Not defined\")\n","        metrics_summary['F1-Score'].append(float(f1_ann) if f1_ann is not None else \"Not defined\")\n","        metrics_summary['ROC AUC'].append(roc_auc_ann if roc_auc_ann is not None else \"Not defined\")\n","\n","        # Print results\n","        print(f\"Keras ANN Accuracy: {accuracy:.4f}\")\n","        if roc_auc_ann is not None:\n","            print(f\"Keras ANN Precision: {precision_ann:.4f}\")\n","            print(f\"Keras ANN Recall: {recall_ann:.4f}\")\n","            print(f\"Keras ANN F1-Score: {f1_ann:.4f}\")\n","            print(f\"Keras ANN ROC AUC: {roc_auc_ann:.4f}\")\n","        else:\n","            print(\"Keras ANN Precision, Recall, F1-Score, ROC AUC: Not defined (only one class present in y_true)\")\n","        print(\"-\" * 50)\n","\n","        if roc_auc_ann is not None:\n","            # Generate the confusion matrix\n","            conf_matrix = confusion_matrix(y_test, y_pred_ann if num_classes == 2 else y_prob_ann.argmax(axis=1))\n","\n","            # Print the confusion matrix\n","            print(\"Confusion Matrix:\")\n","            print(conf_matrix)\n","\n","            # Plot the confusion matrix\n","            disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n","            disp.plot(cmap=plt.cm.Blues)\n","\n","            # Save the confusion matrix plot\n","            conf_matrix_path = os.path.join(output_directory, 'Keras_ANN_confusion_matrix.png')\n","            plt.savefig(conf_matrix_path)\n","            plt.close()\n","\n","            # Add confusion matrix to the document\n","            doc.add_heading('Keras ANN Confusion Matrix', level=2)\n","            doc.add_picture(conf_matrix_path, width=Inches(4.5))\n","        else:\n","            doc.add_heading('Keras ANN Confusion Matrix', level=2)\n","            doc.add_paragraph(\"Not defined (only one class present in y_true)\")\n","\n","        doc.add_paragraph(\"-\" * 50)\n","\n","        # Plotting the metrics for all models\n","        self.plot_metrics(metrics_summary, output_directory, doc)\n","\n","        # Save the document\n","        output_file = os.path.join(output_directory, 'model_evaluation_results_org.docx')\n","        doc.save(output_file)\n","        print(f\"Results saved to {output_file}\")\n","\n","    def run_pipeline(self, target_column_name, test_size):\n","        df = self.load_dataset()\n","        self.data_distribution_plot(df)\n","        self.display_dataset_summary()\n","        df_cleaned = self.clean_data()\n","        self.define_target(df_cleaned, target_column_name)\n","        self.plot_class_distribution(self.y)\n","        self.encode_categorical()\n","        X_train, X_test, y_train, y_test = self.split_data(test_size)\n","        X_train_scaled, X_test_scaled = self.apply_standard_scaler(X_train, X_test)\n","        return X_train_scaled, X_test_scaled, y_train, y_test\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def main():\n","    # Step 1: Define paths for dataset and results\n","    # file_path = '/content/drive/MyDrive/Colab Notebooks/Original_dataset/preprocessed_merged_files.csv' # binary class\n","    file_path = '/content/drive/MyDrive/ResearchTask/Research_Dataset/Copy of preprocessed_merged_files.csv' # multi class\n","    base_output_file = '/content/drive/MyDrive/ResearchTask/'\n","    # Initialize the DataPipeline class\n","    pipeline = DataPipeline(file_path, base_output_file)\n","\n","    target_column_name = input(\"Enter the name of the target column: \")\n","    test_size_input = input(\"Enter the test size as a fraction (e.g., 0.2 for 20%): \")\n","    test_size = float(test_size_input)\n","\n","    dataset_name = \"original dataset\"\n","\n","    # Run the initial pipeline up to data splitting\n","    X_train_scaled, X_test_scaled, y_train, y_test = pipeline.run_pipeline(target_column_name, test_size)\n","\n","    display(HTML(f\"<h3 style='color: red;'>Results for {dataset_name}</h3>\"))\n","\n","    # Define the directory for images\n","    result_images_directory = os.path.join(pipeline.results_dir, 'images')\n","\n","    # Create the directory if it doesn't exist\n","    os.makedirs(result_images_directory, exist_ok=True)\n","\n","    # Document to save the results\n","    doc = Document()\n","\n","    # Train and evaluate the models\n","    pipeline.train_and_evaluate_models(X_train_scaled, X_test_scaled, y_train, y_test, doc, result_images_directory, pipeline.num_classes)\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"markdown","metadata":{},"source":["## Oversampling"]},{"cell_type":"markdown","metadata":{},"source":["Smote"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from imblearn.over_sampling import SMOTE\n","from docx import Document\n","import os\n","import pandas as pd\n","import numpy as np\n","\n","# Function to apply weighted SMOTE\n","def apply_weighted_smote(X, y, sampling_strategy):\n","    # Create SMOTE instance with the custom sampling strategy\n","    smote = SMOTE(\n","            sampling_strategy=sampling_strategy,  # Determines the resampling strategy. Other possible values: 'auto', float, dict, callable, default='auto'\n","            random_state=42,  # Random seed for reproducibility. Other possible values: any integer, default=None\n","            k_neighbors=5,  # Number of nearest neighbors to use for generating the synthetic samples. Other possible values: any positive integer, default=5\n","            n_jobs=None  # Number of parallel jobs to run for nearest neighbors computation. Other possible values: any integer or None, default=None\n","        )\n","\n","    # Apply SMOTE\n","    X_resampled, y_resampled = smote.fit_resample(X, y)\n","\n","    # Check the shape of resampled datasets\n","    print(f\"SMOTE Resampled X shape: {X_resampled.shape}\")\n","    print(f\"SMOTE Resampled y shape: {y_resampled.shape}\")\n","\n","    return X_resampled, y_resampled\n","\n","def smote_pipeline():\n","    # Step 1: Define paths for dataset and results\n","    file_path = '/content/drive/MyDrive/ResearchTask/Research_Dataset/Copy of preprocessed_merged_files.csv' # binary class\n","    base_output_file = '/content/drive/MyDrive/Colab Notebooks/results/'\n","    name = 'smote'\n","\n","    # Define the directory for SMOTE results\n","    base_output_file_smote = os.path.join(base_output_file, name)\n","\n","    # Create the directories if they don't exist\n","    os.makedirs(base_output_file_smote, exist_ok=True)\n","\n","    # Initialize the DataPipeline class\n","    pipeline = DataPipeline(file_path, base_output_file_smote)\n","\n","    target_column_name = input(\"Enter the name of the target column: \")\n","    test_size_input = input(\"Enter the test size as a fraction (e.g., 0.2 for 20%): \")\n","    test_size = float(test_size_input)\n","\n","    # Run the initial pipeline steps\n","    df = pipeline.load_dataset()\n","    df_cleaned = pipeline.clean_data()\n","    X, y = pipeline.define_target(df_cleaned, target_column_name)\n","\n","    # Determine the number of classes\n","    num_classes = len(np.unique(y))\n","\n","    # Split the data\n","    X_train, X_test, y_train, y_test = pipeline.split_data(test_size=test_size)\n","\n","    # Define the custom sampling strategy\n","    if num_classes > 2:  # Multi-class case\n","        class_counts = np.bincount(y_train)\n","        sampling_strategy = {i: max(class_counts) for i in range(num_classes)}\n","\n","        ## Can also define custom sampling strategy for each class\n","        # sampling_strategy = {\n","        #     0: 120,  # Resample class 0 to 120 samples\n","        #     1: 100,  # Resample class 1 to 100 samples\n","        #     2: 8    # Resample class 2 to 80 samples\n","        # }\n","\n","    else:  # Binary case\n","        class_counts = np.bincount(y_train)\n","        sampling_strategy = {0: class_counts[0], 1: class_counts[1] * 2}  # Adjust this strategy as per need\n","\n","\n","    # Apply custom SMOTE\n","    X_smote, y_smote = apply_weighted_smote(X_train, y_train, sampling_strategy)\n","\n","    # Plot the distribution after SMOTE\n","    pipeline.data_distribution_plot(pd.DataFrame(X_smote, columns=X_train.columns))\n","    pipeline.plot_class_distribution(pd.Series(y_smote), plot_title='SMOTE Class Distribution')\n","\n","    # Scale the resampled datasets\n","    X_smote_scaled, X_test_scaled = pipeline.apply_standard_scaler(X_smote, X_test)\n","\n","    # Define the directory for images\n","    result_images_directory_smote = os.path.join(pipeline.results_dir, 'images_SMOTE')\n","\n","    # Create the directories if they don't exist\n","    os.makedirs(result_images_directory_smote, exist_ok=True)\n","\n","    # Document to save the results\n","    doc = Document()\n","\n","    # Train and evaluate models with the SMOTE-applied dataset\n","    pipeline.train_and_evaluate_models(X_smote_scaled, X_test_scaled, y_smote, y_test, doc, result_images_directory_smote, num_classes)\n","\n","    # Save the results to a document\n","    output_file_path = os.path.join(base_output_file_smote, 'model_evaluation_results_with_smote.docx')\n","    doc.save(output_file_path)\n","    print(f\"Results saved to '{output_file_path}'\")\n","\n","if __name__ == \"__main__\":\n","    smote_pipeline()\n"]},{"cell_type":"markdown","metadata":{},"source":["ADASYn"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from imblearn.over_sampling import ADASYN\n","from docx import Document\n","import os\n","import pandas as pd\n","import numpy as np\n","\n","# Function to apply ADASYN\n","def apply_adasyn(X, y, sampling_strategy):\n","    try:\n","        adasyn = ADASYN(\n","            sampling_strategy=sampling_strategy,  # Determines the resampling strategy. Other possible values: 'auto', float, dict, callable, default='auto'\n","            random_state=42,  # Random seed for reproducibility. Other possible values: any integer, default=None\n","            n_neighbors=5,  # Number of nearest neighbors to use to construct synthetic samples. Other possible values: any positive integer, default=5\n","            n_jobs= None  # Number of parallel jobs to run for nearest neighbors computation. Other possible values: any integer or None, default=None\n","        )\n","        X_res, y_res = adasyn.fit_resample(X, y)\n","        print(f\"ADASYN Resampled X shape: {X_res.shape}\")\n","        print(f\"ADASYN Resampled y shape: {y_res.shape}\")\n","\n","        return X_res, y_res\n","\n","    except ValueError as e:\n","        print(f\"Error in ADASYN resampling: {e}\")\n","        print(\"Falling back to the original data without resampling.\")\n","        return X, y\n","\n","def adasyn_pipeline():\n","    # Step 1: Define paths for dataset and results\n","    file_path = '/content/drive/MyDrive/ResearchTask/Research_Dataset/Copy of preprocessed_merged_files.csv'  # Binary\n","    # file_path = '/content/drive/MyDrive/Colab Notebooks/Original_dataset/cyberthreat.csv' # multi class\n","    base_output_file = '/content/drive/MyDrive/Colab Notebooks/results/'\n","    name = 'adasyn'\n","\n","    # Define the directory for ADASYN results\n","    base_output_file_adasyn = os.path.join(base_output_file, name)\n","\n","    # Create the directories if they don't exist\n","    os.makedirs(base_output_file_adasyn, exist_ok=True)\n","\n","    # Initialize the DataPipeline class\n","    pipeline = DataPipeline(file_path, base_output_file_adasyn)\n","\n","    target_column_name = input(\"Enter the name of the target column: \")\n","    test_size_input = input(\"Enter the test size as a fraction (e.g., 0.2 for 20%): \")\n","    test_size = float(test_size_input)\n","\n","    # Run the initial pipeline steps\n","    df = pipeline.load_dataset()\n","    df_cleaned = pipeline.clean_data()\n","    X, y = pipeline.define_target(df_cleaned, target_column_name)\n","\n","    # Determine the number of classes\n","    num_classes = len(np.unique(y))\n","\n","    # Split the data\n","    X_train, X_test, y_train, y_test = pipeline.split_data(test_size=test_size)\n","\n","    # Define the custom sampling strategy\n","    if num_classes > 2:  # Multi-class case\n","        class_counts = np.bincount(y_train)\n","        sampling_strategy = {i: int(max(class_counts) * 1.2) for i in range(num_classes)}\n","\n","        ## Can also define custom sampling strategy for each class\n","        # sampling_strategy = {\n","        #     0: 120,  # Resample class 0 to 120 samples\n","        #     1: 100,  # Resample class 1 to 100 samples\n","        #     2: 80    # Resample class 2 to 80 samples\n","        # }\n","\n","    else:  # Binary case\n","        class_counts = np.bincount(y_train)\n","        sampling_strategy = {0: class_counts[0], 1: int(class_counts[1] * 1.5)}  # Adjust this strategy as per need\n","\n","    # Apply ADASYN with error handling\n","    X_adasyn, y_adasyn = apply_adasyn(X_train, y_train, sampling_strategy)\n","\n","    # Plot the distribution after ADASYN\n","    pipeline.data_distribution_plot(pd.DataFrame(X_adasyn, columns=X_train.columns))\n","    pipeline.plot_class_distribution(pd.Series(y_adasyn), plot_title='ADASYN Class Distribution')\n","\n","    # Scale the resampled datasets\n","    X_adasyn_scaled, X_test_scaled = pipeline.apply_standard_scaler(X_adasyn, X_test)\n","\n","    # Define the directory for images\n","    result_images_directory_adasyn = os.path.join(pipeline.results_dir, 'images_ADASYN')\n","\n","    # Create the directories if they don't exist\n","    os.makedirs(result_images_directory_adasyn, exist_ok=True)\n","\n","    # Document to save the results\n","    doc = Document()\n","\n","    # Train and evaluate models with the ADASYN-applied dataset\n","    pipeline.train_and_evaluate_models(X_adasyn_scaled, X_test_scaled, y_adasyn, y_test, doc, result_images_directory_adasyn, num_classes)\n","\n","    # Save the results to a document\n","    output_file_path = os.path.join(base_output_file_adasyn, 'model_evaluation_results_with_adasyn.docx')\n","    doc.save(output_file_path)\n","    print(f\"Results saved to '{output_file_path}'\")\n","\n","if __name__ == \"__main__\":\n","    adasyn_pipeline()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.6"}},"nbformat":4,"nbformat_minor":0}
