{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip3 install python-docx\n",
        "!pip3 install imbalanced-learn\n",
        "!pip3 install ipython"
      ],
      "metadata": {
        "id": "jg04QNR42nXF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e66a1f8-9816-4ce5-8a1e-a5aa3899334f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (1.1.2)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (5.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.4)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.5.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (7.34.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython) (75.1.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython) (0.2.13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9MQOy2qPiTa",
        "outputId": "b2eed6de-8dbd-4e9a-e9eb-03b551de491e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, label_binarize\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, confusion_matrix, classification_report, roc_curve, ConfusionMatrixDisplay,precision_recall_fscore_support\n",
        "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from google.colab import files, drive\n",
        "from docx import Document\n",
        "from docx.shared import Inches\n",
        "from io import StringIO\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display, HTML\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Set environment variables for reproducibility\n",
        "os.environ['PYTHONHASHSEED'] = '42'\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "\n",
        "# Set random seeds for reproducibility across all libraries\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Suppress all warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "drive.mount('/content/drive/')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naIFbHot5aF2",
        "outputId": "03deb189-4e2b-452b-a7d5-f9abaa8232b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make directory"
      ],
      "metadata": {
        "id": "OUS3AUmSEBbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataPipeline:\n",
        "    def __init__(self, file_path, base_output_file):\n",
        "        self.file_path = file_path\n",
        "        self.base_output_file = base_output_file\n",
        "        self.df = None\n",
        "        self.X = None\n",
        "        self.y = None\n",
        "        self.num_classes = None\n",
        "        # Create the results directory if it doesn't exist\n",
        "        self.results_dir = base_output_file\n",
        "        self.dataset_results_dir = os.path.join(self.results_dir, 'dataset')\n",
        "        self._ensure_directories_exist()\n",
        "\n",
        "    def _ensure_directories_exist(self):\n",
        "        \"\"\"Ensure the results and dataset directories exist.\"\"\"\n",
        "        if not os.path.exists(self.results_dir):\n",
        "            os.makedirs(self.results_dir)\n",
        "            print(f\"Created directory: {self.results_dir}\")\n",
        "\n",
        "        if not os.path.exists(self.dataset_results_dir):\n",
        "            os.makedirs(self.dataset_results_dir)\n",
        "            print(f\"Created directory: {self.dataset_results_dir}\")\n",
        "\n",
        "    def load_dataset(self):\n",
        "        self.df = pd.read_csv(self.file_path)\n",
        "        return self.df\n",
        "\n",
        "    def plot_class_distribution(self, y, plot_title='Class Distribution'):\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        y.value_counts().plot(kind='bar', color='green', alpha=0.7)\n",
        "        plt.title(plot_title)\n",
        "        plt.xlabel('Classes')\n",
        "        plt.ylabel('Frequency')\n",
        "        plot_path = os.path.join(self.dataset_results_dir, f'{plot_title}.png')\n",
        "        plt.savefig(plot_path)\n",
        "        plt.close()\n",
        "\n",
        "    def display_dataset_summary(self):\n",
        "        dataset_output_file_path = os.path.join(self.dataset_results_dir, 'dataset_summary.csv')\n",
        "        with open(dataset_output_file_path, 'w') as f:\n",
        "            f.write('--- Head of Dataset ---\\n')\n",
        "            self.df.head().to_csv(f, index=False)\n",
        "            f.write('\\n')\n",
        "\n",
        "            f.write('--- Describe of Dataset ---\\n')\n",
        "            describe_df = self.df.describe().transpose()\n",
        "            describe_df.to_csv(f)\n",
        "            f.write('\\n')\n",
        "\n",
        "            f.write('--- Info of Dataset ---\\n')\n",
        "            buffer = StringIO()\n",
        "            self.df.info(buf=buffer)\n",
        "            info_lines = buffer.getvalue().splitlines()\n",
        "\n",
        "            for line in info_lines:\n",
        "                if 'Non-Null Count' in line:\n",
        "                    f.write('Column, Non-Null Count, Dtype\\n')\n",
        "                elif '<class' not in line and 'memory' not in line:\n",
        "                    parts = line.split()\n",
        "                    if len(parts) > 3:\n",
        "                        column_name = parts[0]\n",
        "                        non_null_count = parts[-2]\n",
        "                        dtype = parts[-1]\n",
        "                        f.write(f'{column_name}, {non_null_count}, {dtype}\\n')\n",
        "\n",
        "        print(f\"Dataset summary saved to {dataset_output_file_path}\")\n",
        "\n",
        "    def data_distribution_plot(self, df):\n",
        "        dataset_images_directory = os.path.join(self.dataset_results_dir, 'images')\n",
        "\n",
        "        # Ensure the directory exists before saving plots\n",
        "        os.makedirs(dataset_images_directory, exist_ok=True)\n",
        "\n",
        "        numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "        # Plot distribution for numerical columns\n",
        "        for col in numerical_cols:\n",
        "            plt.figure(figsize=(6, 4))\n",
        "            sns.histplot(df[col], kde=True, bins=30)\n",
        "            plt.title(f'Distribution of {col}')\n",
        "            plt.xlabel(col)\n",
        "            plt.ylabel('Frequency')\n",
        "            plot_path = os.path.join(dataset_images_directory, f'Distribution_{col}.png')\n",
        "            plt.savefig(plot_path)\n",
        "            plt.close()  # Close the plot to avoid overlap\n",
        "\n",
        "        categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "        # Plot distribution for categorical columns\n",
        "        for col in categorical_cols:\n",
        "            plt.figure(figsize=(6, 4))\n",
        "            sns.countplot(data=df, x=col)\n",
        "            plt.title(f'Distribution of {col}')\n",
        "            plt.xlabel(col)\n",
        "            plt.ylabel('Frequency')\n",
        "            plt.xticks(rotation=45)\n",
        "            plot_path = os.path.join(dataset_images_directory, f'Distribution_{col}.png')\n",
        "            plt.savefig(plot_path)\n",
        "            plt.close()  # Close the plot to avoid overlap\n",
        "\n",
        "    def clean_data(self):\n",
        "        cleaned_file_path = os.path.join(self.dataset_results_dir, 'cleaned_dataset.csv')\n",
        "        numeric_cols = self.df.select_dtypes(include=['number']).columns\n",
        "        rounded_means = self.df[numeric_cols].mean().round()\n",
        "        self.df[numeric_cols] = self.df[numeric_cols].fillna(rounded_means)\n",
        "\n",
        "        categorical_cols = self.df.select_dtypes(include=['object']).columns\n",
        "        for col in categorical_cols:\n",
        "            self.df[col] = self.df[col].fillna(self.df[col].mode()[0])\n",
        "\n",
        "        self.df.to_csv(cleaned_file_path, index=False)\n",
        "        print(f\"Cleaned dataset saved to {cleaned_file_path}\")\n",
        "        return self.df\n",
        "\n",
        "    def define_target(self, df_cleaned, target_column_name):\n",
        "        self.y = df_cleaned[target_column_name]\n",
        "        self.X = df_cleaned.drop(columns=[target_column_name])\n",
        "        self.num_classes = len(np.unique(self.y))\n",
        "        print(f\"Detected {self.num_classes} unique classes in the target column.\")\n",
        "        return self.X, self.y\n",
        "\n",
        "    def encode_categorical(self):\n",
        "        label_encoders = {}\n",
        "\n",
        "        # Check for categorical columns in X and apply LabelEncoder only if categorical columns exist\n",
        "        categorical_columns = self.X.select_dtypes(include=['object']).columns\n",
        "\n",
        "        # If categorical columns exist, apply LabelEncoder\n",
        "        if not categorical_columns.empty:\n",
        "            for col in categorical_columns:\n",
        "                label_encoders[col] = LabelEncoder()\n",
        "                self.X[col] = label_encoders[col].fit_transform(self.X[col])\n",
        "\n",
        "        # Encode the target variable (y) if necessary\n",
        "        if self.y.dtype == 'object' or self.y.dtype.name == 'category':\n",
        "            y_encoder = LabelEncoder()\n",
        "            y_encoded = y_encoder.fit_transform(self.y)\n",
        "        else:\n",
        "            y_encoded = self.y  # If y is numeric, no encoding is needed\n",
        "\n",
        "        # Return the updated X and y_encoded\n",
        "        return self.X, y_encoded\n",
        "\n",
        "    def split_data(self, test_size, random_state=42):\n",
        "        X_encoded, y_encoded = self.encode_categorical()\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=test_size, random_state=random_state, stratify=self.y)\n",
        "        return X_train, X_test, y_train, y_test\n",
        "\n",
        "    def apply_standard_scaler(self, X_train, X_test):\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "        return X_train_scaled, X_test_scaled\n",
        "\n",
        "    def tune_model_params(self, default_model):\n",
        "        param_defaults = {\n",
        "        \"RandomForestClassifier\": {\n",
        "            \"n_estimators\": 200,  # Number of trees in the forest. Other possible values: any positive integer, default=100\n",
        "            \"max_depth\": 25,  # Maximum depth of the tree. Other possible values: any positive integer, default=None\n",
        "            \"min_samples_split\": 10,  # Minimum number of samples required to split an internal node. Other possible values: any positive integer, default=2\n",
        "            \"min_samples_leaf\": 4,  # Minimum number of samples required to be at a leaf node. Other possible values: any positive integer, default=1\n",
        "            \"criterion\": 'gini',  # Function to measure the quality of a split. Other possible values: 'entropy', 'log_loss', default='gini'\n",
        "            \"random_state\": 52  # Random seed for reproducibility. Other possible values: any integer, default=None\n",
        "        },\n",
        "        \"SVC\": {\n",
        "            \"C\": 1.0,  # Regularization parameter. Other possible values: any positive float, default=1.0\n",
        "            \"kernel\": 'rbf',  # Kernel type. Other possible values: 'linear', 'poly', 'sigmoid', 'precomputed', default='rbf'\n",
        "            \"gamma\": 'scale',  # Kernel coefficient. Other possible values: 'auto', any positive float, default='scale'\n",
        "            \"degree\": 3,  # Degree of the polynomial kernel function ('poly'). Other possible values: any positive integer, default=3\n",
        "            \"random_state\": 52  # Random seed for reproducibility. Other possible values: any integer, default=None\n",
        "        },\n",
        "        \"DecisionTreeClassifier\": {\n",
        "            \"max_depth\": 300,  # Maximum depth of the tree. Other possible values: any positive integer, default=None\n",
        "            \"min_samples_split\": 20,  # Minimum number of samples required to split an internal node. Other possible values: any positive integer, default=2\n",
        "            \"min_samples_leaf\": 8,  # Minimum number of samples required to be at a leaf node. Other possible values: any positive integer, default=1\n",
        "            \"criterion\": 'entropy',  # Function to measure the quality of a split. Other possible values: 'gini', default='gini'\n",
        "            \"random_state\": 52  # Random seed for reproducibility. Other possible values: any integer, default=None\n",
        "        },\n",
        "        \"KNeighborsClassifier\": {\n",
        "            \"n_neighbors\": 10,  # Number of neighbors to use. Other possible values: any positive integer, default=5\n",
        "            \"weights\": 'distance',  # Weight function used in prediction. Other possible values: 'uniform', default='uniform'\n",
        "            \"algorithm\": 'auto',  # Algorithm used to compute the nearest neighbors. Other possible values: 'ball_tree', 'kd_tree', 'brute', default='auto'\n",
        "            \"leaf_size\": 40  # Leaf size passed to BallTree or KDTree. Other possible values: any positive integer, default=30\n",
        "        },\n",
        "        \"LogisticRegression\": {\n",
        "            \"C\": 0.3,  # Inverse of regularization strength. Other possible values: any positive float, default=1.0\n",
        "            \"solver\": 'liblinear',  # Algorithm to use in the optimization problem. Other possible values: 'newton-cg', 'lbfgs', 'sag', 'saga', default='lbfgs'\n",
        "            \"max_iter\": 300,  # Maximum number of iterations. Other possible values: any positive integer, default=100\n",
        "            \"penalty\": 'l2',  # Norm used in the penalization. Other possible values: 'l1', 'elasticnet', None, default='l2'\n",
        "            \"random_state\": 52  # Random seed for reproducibility. Other possible values: any integer, default=None\n",
        "        }\n",
        "    }\n",
        "\n",
        "        model_name = default_model.__class__.__name__\n",
        "        if model_name in param_defaults:\n",
        "            params = param_defaults[model_name]\n",
        "            return default_model.set_params(**params)\n",
        "        else:\n",
        "            print(f\"\\nNo specific tuning parameters available for {model_name}. Using default parameters.\")\n",
        "            return default_model\n",
        "    from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "    def cross_validate(self, model, X, y, n_splits=5):\n",
        "        skf = StratifiedKFold(n_splits=n_splits)\n",
        "        accuracy_scores = []\n",
        "        precision_scores = []\n",
        "        recall_scores = []\n",
        "        f1_scores = []\n",
        "\n",
        "        for train_index, test_index in skf.split(X, y):\n",
        "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "            model.fit(X_train, y_train)\n",
        "            y_pred = model.predict(X_test)\n",
        "\n",
        "            accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
        "            precision_scores.append(precision_score(y_test, y_pred, average='weighted'))\n",
        "            recall_scores.append(recall_score(y_test, y_pred, average='weighted'))\n",
        "            f1_scores.append(f1_score(y_test, y_pred, average='weighted'))\n",
        "\n",
        "        print(f\"Accuracy: {np.mean(accuracy_scores):.4f}\")\n",
        "        print(f\"Precision: {np.mean(precision_scores):.4f}\")\n",
        "        print(f\"Recall: {np.mean(recall_scores):.4f}\")\n",
        "        print(f\"F1-Score: {np.mean(f1_scores):.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "    def plot_metrics(self, metrics_summary, output_directory, doc):\n",
        "        models = metrics_summary['Model']\n",
        "        metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "\n",
        "        for metric in metrics:\n",
        "            plt.figure()\n",
        "            plt.bar(models, metrics_summary[metric], color='skyblue')\n",
        "            plt.xlabel('Model')\n",
        "            plt.ylabel(metric)\n",
        "            plt.title(f'Model Comparison: {metric}')\n",
        "            metric_plot_path = os.path.join(output_directory, f'{metric}_comparison.png')\n",
        "            plt.savefig(metric_plot_path)\n",
        "            plt.close()\n",
        "\n",
        "            # Add the plot to the document\n",
        "            doc.add_heading(f'{metric} Comparison', level=2)\n",
        "            doc.add_picture(metric_plot_path, width=Inches(4.5))\n",
        "            doc.add_paragraph(\"-\" * 50)\n",
        "\n",
        "    def train_and_evaluate_models(self, X_train_scaled, X_test_scaled, y_train, y_test, doc, output_directory, num_classes):\n",
        "       # Here you can add more algorithms\n",
        "        models = {\n",
        "            \"Random Forest\": self.tune_model_params(RandomForestClassifier()),\n",
        "            \"SVM\": self.tune_model_params(SVC(probability=True)),\n",
        "            \"Decision Tree\": self.tune_model_params(DecisionTreeClassifier()),\n",
        "            \"KNN\": self.tune_model_params(KNeighborsClassifier()),\n",
        "            \"Logistic Regression\": self.tune_model_params(LogisticRegression())\n",
        "        }\n",
        "\n",
        "        metrics_summary = {\n",
        "            'Model': [],\n",
        "            'Accuracy': [],\n",
        "            'Precision': [],\n",
        "            'Recall': [],\n",
        "            'F1-Score': [],\n",
        "            'ROC AUC': []\n",
        "        }\n",
        "\n",
        "        for name, model in models.items():\n",
        "            print(f\"\\nTraining and evaluating {name}...\")\n",
        "            model.fit(X_train_scaled, y_train)\n",
        "            y_pred = model.predict(X_test_scaled)\n",
        "            y_prob = model.predict_proba(X_test_scaled) if hasattr(model, \"predict_proba\") else model.decision_function(X_test_scaled)\n",
        "\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
        "\n",
        "            classification_rep = classification_report(y_test, y_pred)\n",
        "            print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
        "            print(f\"{name} Classification Report:\\n{classification_rep}\\n\")\n",
        "\n",
        "            # Initialize roc_auc as None\n",
        "            roc_auc = None\n",
        "\n",
        "            # Calculate ROC AUC for binary and multi-class cases\n",
        "            if num_classes > 2:\n",
        "                # Multi-class case\n",
        "                y_test_bin = label_binarize(y_test, classes=np.arange(num_classes))\n",
        "                try:\n",
        "                    roc_auc = roc_auc_score(y_test_bin, y_prob, multi_class='ovr')\n",
        "                except ValueError:\n",
        "                    roc_auc = None  # Handle any issues during calculation\n",
        "            else:\n",
        "                # Binary case\n",
        "                try:\n",
        "                    roc_auc = roc_auc_score(y_test, y_prob[:, 1])  # Assuming y_prob is the probability for the positive class\n",
        "                except ValueError:\n",
        "                    roc_auc = None  # Handle any issues during calculation\n",
        "\n",
        "            # Print the ROC AUC result\n",
        "            if roc_auc is not None:\n",
        "                print(f\"{name} ROC AUC: {roc_auc:.4f}\")\n",
        "            else:\n",
        "                print(f\"{name} ROC AUC: Not defined\")\n",
        "\n",
        "\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "            # Save metrics to summary\n",
        "            metrics_summary['Model'].append(name)\n",
        "            metrics_summary['Accuracy'].append(accuracy)\n",
        "            metrics_summary['Precision'].append(precision)\n",
        "            metrics_summary['Recall'].append(recall)\n",
        "            metrics_summary['F1-Score'].append(f1)\n",
        "            metrics_summary['ROC AUC'].append(roc_auc if roc_auc is not None else \"Not defined\")\n",
        "\n",
        "            # Plot ROC curve if ROC AUC is defined\n",
        "            if roc_auc is not None:\n",
        "                if num_classes == 2:\n",
        "                    fpr, tpr, _ = roc_curve(y_test, y_prob[:, 1])\n",
        "                else:\n",
        "                    fpr, tpr, _ = roc_curve(y_test_bin.ravel(), y_prob.ravel())\n",
        "                plt.figure()\n",
        "                plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "                plt.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
        "                plt.xlim([0.0, 1.0])\n",
        "                plt.ylim([0.0, 1.05])\n",
        "                plt.xlabel('False Positive Rate')\n",
        "                plt.ylabel('True Positive Rate')\n",
        "                plt.title(f'{name} ROC Curve')\n",
        "                plt.legend(loc=\"lower right\")\n",
        "\n",
        "                # Save the plot\n",
        "                roc_curve_path = os.path.join(output_directory, f'{name}_roc_curve.png')\n",
        "                plt.savefig(roc_curve_path)\n",
        "                plt.close()\n",
        "\n",
        "                # Add results and ROC curve image to the document\n",
        "                doc.add_heading(f'{name} Results', level=2)\n",
        "                doc.add_paragraph(f\"Accuracy: {accuracy:.4f}\")\n",
        "                doc.add_paragraph(\"Classification Report:\")\n",
        "                doc.add_paragraph(classification_rep)\n",
        "                doc.add_paragraph(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "                doc.add_picture(roc_curve_path, width=Inches(4.5))\n",
        "            else:\n",
        "                doc.add_heading(f'{name} Results', level=2)\n",
        "                doc.add_paragraph(f\"Accuracy: {accuracy:.4f}\")\n",
        "                doc.add_paragraph(\"Classification Report:\")\n",
        "                doc.add_paragraph(classification_rep)\n",
        "                doc.add_paragraph(\"ROC AUC: Not defined (only one class present in y_true)\")\n",
        "\n",
        "            doc.add_paragraph(\"-\" * 50)\n",
        "\n",
        "        # -------------------- Add Keras ANN Below -------------------- #\n",
        "\n",
        "        print(\"\\nTraining and evaluating Keras ANN...\")\n",
        "\n",
        "        ann_model = Sequential()\n",
        "        ann_model.add(Dense(units=64, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
        "        ann_model.add(Dense(units=32, activation='relu'))\n",
        "\n",
        "        if num_classes > 2:\n",
        "            ann_model.add(Dense(units=num_classes, activation='softmax'))  # Multiclass classification\n",
        "            loss_function = 'sparse_categorical_crossentropy'\n",
        "        else:\n",
        "            ann_model.add(Dense(units=1, activation='sigmoid'))  # Binary classification\n",
        "            loss_function = 'binary_crossentropy'\n",
        "\n",
        "        ann_model.compile(\n",
        "            optimizer='adam',\n",
        "            loss=loss_function,\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        ann_model.fit(\n",
        "            X_train_scaled, y_train,\n",
        "            epochs=10,\n",
        "            batch_size=32,\n",
        "            validation_data=(X_test_scaled, y_test),\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        loss, accuracy = ann_model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "\n",
        "        # Evaluate model based on the number of classes\n",
        "        if len(np.unique(y_test)) == 1:\n",
        "            print(\"Keras ANN ROC AUC: Not defined (only one class present in y_true)\")\n",
        "            roc_auc_ann = None  # ROC AUC cannot be defined\n",
        "            precision_ann = None\n",
        "            recall_ann = None\n",
        "            f1_ann = None\n",
        "        else:\n",
        "            if num_classes > 2:\n",
        "                y_prob_ann = ann_model.predict(X_test_scaled)\n",
        "                y_test_bin = to_categorical(y_test, num_classes=num_classes)\n",
        "                roc_auc_ann = roc_auc_score(y_test_bin, y_prob_ann, multi_class='ovr')\n",
        "                precision_ann = precision_score(y_test_bin.argmax(axis=1), y_prob_ann.argmax(axis=1), average='weighted')\n",
        "                recall_ann = recall_score(y_test_bin.argmax(axis=1), y_prob_ann.argmax(axis=1), average='weighted')\n",
        "                f1_ann = f1_score(y_test_bin.argmax(axis=1), y_prob_ann.argmax(axis=1), average='weighted')\n",
        "            else:\n",
        "                y_prob_ann = ann_model.predict(X_test_scaled).ravel()\n",
        "                y_pred_ann = (y_prob_ann > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
        "                roc_auc_ann = roc_auc_score(y_test, y_prob_ann)\n",
        "                precision_ann = precision_score(y_test, y_pred_ann)\n",
        "                recall_ann = recall_score(y_test, y_pred_ann)\n",
        "                f1_ann = f1_score(y_test, y_pred_ann)\n",
        "\n",
        "        # Append ANN metrics to summary\n",
        "        metrics_summary['Model'].append(\"Keras ANN\")\n",
        "        metrics_summary['Accuracy'].append(float(accuracy))\n",
        "        metrics_summary['Precision'].append(float(precision_ann) if precision_ann is not None else \"Not defined\")\n",
        "        metrics_summary['Recall'].append(float(recall_ann) if recall_ann is not None else \"Not defined\")\n",
        "        metrics_summary['F1-Score'].append(float(f1_ann) if f1_ann is not None else \"Not defined\")\n",
        "        metrics_summary['ROC AUC'].append(roc_auc_ann if roc_auc_ann is not None else \"Not defined\")\n",
        "\n",
        "        # Print results\n",
        "        print(f\"Keras ANN Accuracy: {accuracy:.4f}\")\n",
        "        if roc_auc_ann is not None:\n",
        "            print(f\"Keras ANN Precision: {precision_ann:.4f}\")\n",
        "            print(f\"Keras ANN Recall: {recall_ann:.4f}\")\n",
        "            print(f\"Keras ANN F1-Score: {f1_ann:.4f}\")\n",
        "            print(f\"Keras ANN ROC AUC: {roc_auc_ann:.4f}\")\n",
        "        else:\n",
        "            print(\"Keras ANN Precision, Recall, F1-Score, ROC AUC: Not defined (only one class present in y_true)\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        if roc_auc_ann is not None:\n",
        "            # Generate the confusion matrix\n",
        "            conf_matrix = confusion_matrix(y_test, y_pred_ann if num_classes == 2 else y_prob_ann.argmax(axis=1))\n",
        "\n",
        "            # Print the confusion matrix\n",
        "            print(\"Confusion Matrix:\")\n",
        "            print(conf_matrix)\n",
        "\n",
        "            # Plot the confusion matrix\n",
        "            disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n",
        "            disp.plot(cmap=plt.cm.Blues)\n",
        "\n",
        "            # Save the confusion matrix plot\n",
        "            conf_matrix_path = os.path.join(output_directory, 'Keras_ANN_confusion_matrix.png')\n",
        "            plt.savefig(conf_matrix_path)\n",
        "            plt.close()\n",
        "\n",
        "            # Add confusion matrix to the document\n",
        "            doc.add_heading('Keras ANN Confusion Matrix', level=2)\n",
        "            doc.add_picture(conf_matrix_path, width=Inches(4.5))\n",
        "        else:\n",
        "            doc.add_heading('Keras ANN Confusion Matrix', level=2)\n",
        "            doc.add_paragraph(\"Not defined (only one class present in y_true)\")\n",
        "\n",
        "        doc.add_paragraph(\"-\" * 50)\n",
        "\n",
        "        # Plotting the metrics for all models\n",
        "        self.plot_metrics(metrics_summary, output_directory, doc)\n",
        "\n",
        "        # Save the document\n",
        "        output_file = os.path.join(output_directory, 'model_evaluation_results_org.docx')\n",
        "        doc.save(output_file)\n",
        "        print(f\"Results saved to {output_file}\")\n",
        "\n",
        "    def run_pipeline(self, target_column_name, test_size):\n",
        "        df = self.load_dataset()\n",
        "        self.data_distribution_plot(df)\n",
        "        self.display_dataset_summary()\n",
        "        df_cleaned = self.clean_data()\n",
        "        self.define_target(df_cleaned, target_column_name)\n",
        "        self.plot_class_distribution(self.y)\n",
        "        self.encode_categorical()\n",
        "        X_train, X_test, y_train, y_test = self.split_data(test_size)\n",
        "        X_train_scaled, X_test_scaled = self.apply_standard_scaler(X_train, X_test)\n",
        "        return X_train_scaled, X_test_scaled, y_train, y_test\n",
        "\n"
      ],
      "metadata": {
        "id": "zEJzCyxX5aZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Step 1: Define paths for dataset and results\n",
        "    file_path = '/content/drive/MyDrive/ResearchTask/Research_Dataset/Copy of preprocessed_merged_files.csv' # multi class\n",
        "    base_output_file = '/content/drive/MyDrive/ResearchTask/results'\n",
        "    # Initialize the DataPipeline class\n",
        "    pipeline = DataPipeline(file_path, base_output_file)\n",
        "\n",
        "    # Input for target column and test size\n",
        "    target_column_name = input(\"Enter the name of the target column: \")\n",
        "    test_size_input = input(\"Enter the test size as a fraction (e.g., 0.2 for 20%): \")\n",
        "    test_size = float(test_size_input)\n",
        "\n",
        "    dataset_name = \"original dataset\"\n",
        "\n",
        "    # Load the dataset (assuming the DataPipeline class has a method for loading data)\n",
        "    dataset = pipeline.load_dataset()\n",
        "\n",
        "    # Analyze class distribution for imbalance\n",
        "    print(\"\\nAnalyzing class distribution...\")\n",
        "    analyze_class_distribution(dataset, target_column_name)\n",
        "\n",
        "    # Run the pipeline up to data splitting\n",
        "    X_train_scaled, X_test_scaled, y_train, y_test = pipeline.run_pipeline(target_column_name, test_size)\n",
        "\n",
        "    display(HTML(f\"<h3 style='color: red;'>Results for {dataset_name}</h3>\"))\n",
        "\n",
        "    # Define the directory for images\n",
        "    result_images_directory = os.path.join(pipeline.results_dir, 'images')\n",
        "\n",
        "    # Create the directory if it doesn't exist\n",
        "    os.makedirs(result_images_directory, exist_ok=True)\n",
        "\n",
        "    # Document to save the results\n",
        "    doc = Document()\n",
        "\n",
        "    # Train and evaluate the models\n",
        "    pipeline.train_and_evaluate_models(X_train_scaled, X_test_scaled, y_train, y_test, doc, result_images_directory, pipeline.num_classes)\n",
        "\n",
        "def analyze_class_distribution(dataset, target_column):\n",
        "    \"\"\"\n",
        "    Analyzes and visualizes class distribution to check for imbalance.\n",
        "\n",
        "    Parameters:\n",
        "        dataset (pd.DataFrame): The input dataset.\n",
        "        target_column (str): The name of the target column.\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    class_counts = dataset[target_column].value_counts()\n",
        "    class_percentages = (class_counts / len(dataset)) * 100\n",
        "\n",
        "    print(\"\\nClass Distribution:\")\n",
        "    print(class_counts)\n",
        "    print(\"\\nClass Percentages:\")\n",
        "    print(class_percentages)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "J4ZkGgoBXVR6",
        "outputId": "0a480c56-d0c8-4704-9655-24d9cfc1cf34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created directory: /content/drive/MyDrive/ResearchTask/results/dataset\n",
            "Enter the name of the target column: class\n",
            "Enter the test size as a fraction (e.g., 0.2 for 20%): 0.2\n",
            "\n",
            "Analyzing class distribution...\n",
            "\n",
            "Class Distribution:\n",
            "class\n",
            "0    51\n",
            "1    48\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Class Percentages:\n",
            "class\n",
            "0    51.515152\n",
            "1    48.484848\n",
            "Name: count, dtype: float64\n",
            "Dataset summary saved to /content/drive/MyDrive/ResearchTask/results/dataset/dataset_summary.csv\n",
            "Cleaned dataset saved to /content/drive/MyDrive/ResearchTask/results/dataset/cleaned_dataset.csv\n",
            "Detected 2 unique classes in the target column.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h3 style='color: red;'>Results for original dataset</h3>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training and evaluating Random Forest...\n",
            "Random Forest Accuracy: 0.9500\n",
            "Random Forest Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      1.00      0.95        10\n",
            "           1       1.00      0.90      0.95        10\n",
            "\n",
            "    accuracy                           0.95        20\n",
            "   macro avg       0.95      0.95      0.95        20\n",
            "weighted avg       0.95      0.95      0.95        20\n",
            "\n",
            "\n",
            "Random Forest ROC AUC: 0.9900\n",
            "--------------------------------------------------\n",
            "\n",
            "Training and evaluating SVM...\n",
            "SVM Accuracy: 1.0000\n",
            "SVM Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00        10\n",
            "\n",
            "    accuracy                           1.00        20\n",
            "   macro avg       1.00      1.00      1.00        20\n",
            "weighted avg       1.00      1.00      1.00        20\n",
            "\n",
            "\n",
            "SVM ROC AUC: 1.0000\n",
            "--------------------------------------------------\n",
            "\n",
            "Training and evaluating Decision Tree...\n",
            "Decision Tree Accuracy: 0.9500\n",
            "Decision Tree Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      1.00      0.95        10\n",
            "           1       1.00      0.90      0.95        10\n",
            "\n",
            "    accuracy                           0.95        20\n",
            "   macro avg       0.95      0.95      0.95        20\n",
            "weighted avg       0.95      0.95      0.95        20\n",
            "\n",
            "\n",
            "Decision Tree ROC AUC: 0.9500\n",
            "--------------------------------------------------\n",
            "\n",
            "Training and evaluating KNN...\n",
            "KNN Accuracy: 0.9500\n",
            "KNN Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.90      0.95        10\n",
            "           1       0.91      1.00      0.95        10\n",
            "\n",
            "    accuracy                           0.95        20\n",
            "   macro avg       0.95      0.95      0.95        20\n",
            "weighted avg       0.95      0.95      0.95        20\n",
            "\n",
            "\n",
            "KNN ROC AUC: 0.9900\n",
            "--------------------------------------------------\n",
            "\n",
            "Training and evaluating Logistic Regression...\n",
            "Logistic Regression Accuracy: 1.0000\n",
            "Logistic Regression Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00        10\n",
            "\n",
            "    accuracy                           1.00        20\n",
            "   macro avg       1.00      1.00      1.00        20\n",
            "weighted avg       1.00      1.00      1.00        20\n",
            "\n",
            "\n",
            "Logistic Regression ROC AUC: 1.0000\n",
            "--------------------------------------------------\n",
            "\n",
            "Training and evaluating Keras ANN...\n",
            "Epoch 1/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 106ms/step - accuracy: 0.5022 - loss: 0.7916 - val_accuracy: 0.4500 - val_loss: 0.7439\n",
            "Epoch 2/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5188 - loss: 0.6802 - val_accuracy: 0.4500 - val_loss: 0.6502\n",
            "Epoch 3/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5622 - loss: 0.5922 - val_accuracy: 0.5500 - val_loss: 0.5703\n",
            "Epoch 4/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6572 - loss: 0.5164 - val_accuracy: 0.7500 - val_loss: 0.4985\n",
            "Epoch 5/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8649 - loss: 0.4509 - val_accuracy: 0.8500 - val_loss: 0.4339\n",
            "Epoch 6/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9395 - loss: 0.3949 - val_accuracy: 0.9000 - val_loss: 0.3788\n",
            "Epoch 7/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9576 - loss: 0.3444 - val_accuracy: 0.9500 - val_loss: 0.3299\n",
            "Epoch 8/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9576 - loss: 0.2995 - val_accuracy: 1.0000 - val_loss: 0.2866\n",
            "Epoch 9/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9756 - loss: 0.2598 - val_accuracy: 1.0000 - val_loss: 0.2487\n",
            "Epoch 10/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9756 - loss: 0.2244 - val_accuracy: 1.0000 - val_loss: 0.2157\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
            "Keras ANN Accuracy: 1.0000\n",
            "Keras ANN Precision: 1.0000\n",
            "Keras ANN Recall: 1.0000\n",
            "Keras ANN F1-Score: 1.0000\n",
            "Keras ANN ROC AUC: 1.0000\n",
            "--------------------------------------------------\n",
            "Confusion Matrix:\n",
            "[[10  0]\n",
            " [ 0 10]]\n",
            "Results saved to /content/drive/MyDrive/ResearchTask/results/images/model_evaluation_results_org.docx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Step 1: Define paths for dataset and results\n",
        "    # file_path = '/content/drive/MyDrive/Colab Notebooks/Original_dataset/preprocessed_merged_files.csv' # binary class\n",
        "    file_path = '/content/drive/MyDrive/ResearchTask/Research_Dataset/Copy of preprocessed_merged_files.csv' # multi class\n",
        "    base_output_file = '/content/drive/MyDrive/ResearchTask/results/'\n",
        "    # Initialize the DataPipeline class\n",
        "    pipeline = DataPipeline(file_path, base_output_file)\n",
        "\n",
        "    target_column_name = input(\"Enter the name of the target column: \")\n",
        "    test_size_input = input(\"Enter the test size as a fraction (e.g., 0.2 for 20%): \")\n",
        "    test_size = float(test_size_input)\n",
        "\n",
        "    dataset_name = \"original dataset\"\n",
        "\n",
        "    # Run the initial pipeline up to data splitting\n",
        "    X_train_scaled, X_test_scaled, y_train, y_test = pipeline.run_pipeline(target_column_name, test_size)\n",
        "\n",
        "    display(HTML(f\"<h3 style='color: red;'>Results for {dataset_name}</h3>\"))\n",
        "\n",
        "    # Define the directory for images\n",
        "    result_images_directory = os.path.join(pipeline.results_dir, 'images')\n",
        "\n",
        "    # Create the directory if it doesn't exist\n",
        "    os.makedirs(result_images_directory, exist_ok=True)\n",
        "\n",
        "    # Document to save the results\n",
        "    doc = Document()\n",
        "\n",
        "    # Train and evaluate the models\n",
        "    pipeline.train_and_evaluate_models(X_train_scaled, X_test_scaled, y_train, y_test, doc, result_images_directory, pipeline.num_classes)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Yia3Nnp92SN7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "13be436d-b365-4ea6-90c8-43284b195528"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the name of the target column: class\n",
            "Enter the test size as a fraction (e.g., 0.2 for 20%): 0.2\n",
            "Dataset summary saved to /content/drive/MyDrive/ResearchTask/dataset/dataset_summary.csv\n",
            "Cleaned dataset saved to /content/drive/MyDrive/ResearchTask/dataset/cleaned_dataset.csv\n",
            "Detected 2 unique classes in the target column.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h3 style='color: red;'>Results for original dataset</h3>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training and evaluating Random Forest...\n",
            "Random Forest Accuracy: 0.9500\n",
            "Random Forest Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      1.00      0.95        10\n",
            "           1       1.00      0.90      0.95        10\n",
            "\n",
            "    accuracy                           0.95        20\n",
            "   macro avg       0.95      0.95      0.95        20\n",
            "weighted avg       0.95      0.95      0.95        20\n",
            "\n",
            "\n",
            "Random Forest ROC AUC: 0.9900\n",
            "--------------------------------------------------\n",
            "\n",
            "Training and evaluating SVM...\n",
            "SVM Accuracy: 1.0000\n",
            "SVM Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00        10\n",
            "\n",
            "    accuracy                           1.00        20\n",
            "   macro avg       1.00      1.00      1.00        20\n",
            "weighted avg       1.00      1.00      1.00        20\n",
            "\n",
            "\n",
            "SVM ROC AUC: 1.0000\n",
            "--------------------------------------------------\n",
            "\n",
            "Training and evaluating Decision Tree...\n",
            "Decision Tree Accuracy: 0.9500\n",
            "Decision Tree Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      1.00      0.95        10\n",
            "           1       1.00      0.90      0.95        10\n",
            "\n",
            "    accuracy                           0.95        20\n",
            "   macro avg       0.95      0.95      0.95        20\n",
            "weighted avg       0.95      0.95      0.95        20\n",
            "\n",
            "\n",
            "Decision Tree ROC AUC: 0.9500\n",
            "--------------------------------------------------\n",
            "\n",
            "Training and evaluating KNN...\n",
            "KNN Accuracy: 0.9500\n",
            "KNN Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.90      0.95        10\n",
            "           1       0.91      1.00      0.95        10\n",
            "\n",
            "    accuracy                           0.95        20\n",
            "   macro avg       0.95      0.95      0.95        20\n",
            "weighted avg       0.95      0.95      0.95        20\n",
            "\n",
            "\n",
            "KNN ROC AUC: 0.9900\n",
            "--------------------------------------------------\n",
            "\n",
            "Training and evaluating Logistic Regression...\n",
            "Logistic Regression Accuracy: 1.0000\n",
            "Logistic Regression Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00        10\n",
            "\n",
            "    accuracy                           1.00        20\n",
            "   macro avg       1.00      1.00      1.00        20\n",
            "weighted avg       1.00      1.00      1.00        20\n",
            "\n",
            "\n",
            "Logistic Regression ROC AUC: 1.0000\n",
            "--------------------------------------------------\n",
            "\n",
            "Training and evaluating Keras ANN...\n",
            "Epoch 1/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - accuracy: 0.6182 - loss: 0.6614 - val_accuracy: 0.8000 - val_loss: 0.6199\n",
            "Epoch 2/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8664 - loss: 0.5715 - val_accuracy: 0.9000 - val_loss: 0.5416\n",
            "Epoch 3/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9898 - loss: 0.4991 - val_accuracy: 0.9000 - val_loss: 0.4749\n",
            "Epoch 4/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9898 - loss: 0.4375 - val_accuracy: 0.9500 - val_loss: 0.4159\n",
            "Epoch 5/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9898 - loss: 0.3835 - val_accuracy: 1.0000 - val_loss: 0.3639\n",
            "Epoch 6/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.3358 - val_accuracy: 1.0000 - val_loss: 0.3181\n",
            "Epoch 7/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.2939 - val_accuracy: 1.0000 - val_loss: 0.2775\n",
            "Epoch 8/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.2566 - val_accuracy: 1.0000 - val_loss: 0.2415\n",
            "Epoch 9/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.2229 - val_accuracy: 1.0000 - val_loss: 0.2094\n",
            "Epoch 10/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.1931 - val_accuracy: 1.0000 - val_loss: 0.1815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7d4cca643f40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
            "Keras ANN Accuracy: 1.0000\n",
            "Keras ANN Precision: 1.0000\n",
            "Keras ANN Recall: 1.0000\n",
            "Keras ANN F1-Score: 1.0000\n",
            "Keras ANN ROC AUC: 1.0000\n",
            "--------------------------------------------------\n",
            "Confusion Matrix:\n",
            "[[10  0]\n",
            " [ 0 10]]\n",
            "Results saved to /content/drive/MyDrive/ResearchTask/images/model_evaluation_results_org.docx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Oversampling"
      ],
      "metadata": {
        "id": "z0fe_uNNeIYX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SMOTE"
      ],
      "metadata": {
        "id": "zM2Ohn3ABBAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from docx import Document\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Function to apply weighted SMOTE\n",
        "def apply_weighted_smote(X, y, sampling_strategy):\n",
        "    # Create SMOTE instance with the custom sampling strategy\n",
        "    smote = SMOTE(\n",
        "            sampling_strategy=sampling_strategy,  # Determines the resampling strategy. Other possible values: 'auto', float, dict, callable, default='auto'\n",
        "            random_state=42,  # Random seed for reproducibility. Other possible values: any integer, default=None\n",
        "            k_neighbors=5,  # Number of nearest neighbors to use for generating the synthetic samples. Other possible values: any positive integer, default=5\n",
        "            n_jobs=None  # Number of parallel jobs to run for nearest neighbors computation. Other possible values: any integer or None, default=None\n",
        "        )\n",
        "\n",
        "    # Apply SMOTE\n",
        "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "    # Check the shape of resampled datasets\n",
        "    print(f\"SMOTE Resampled X shape: {X_resampled.shape}\")\n",
        "    print(f\"SMOTE Resampled y shape: {y_resampled.shape}\")\n",
        "\n",
        "    return X_resampled, y_resampled\n",
        "\n",
        "def smote_pipeline():\n",
        "    # Step 1: Define paths for dataset and results\n",
        "    file_path = '/content/drive/MyDrive/ResearchTask/Research_Dataset/Copy of preprocessed_merged_files.csv' # binary class\n",
        "    base_output_file = '/content/drive/MyDrive/Colab Notebooks/results/'\n",
        "    name = 'smote'\n",
        "\n",
        "    # Define the directory for SMOTE results\n",
        "    base_output_file_smote = os.path.join(base_output_file, name)\n",
        "\n",
        "    # Create the directories if they don't exist\n",
        "    os.makedirs(base_output_file_smote, exist_ok=True)\n",
        "\n",
        "    # Initialize the DataPipeline class\n",
        "    pipeline = DataPipeline(file_path, base_output_file_smote)\n",
        "\n",
        "    target_column_name = input(\"Enter the name of the target column: \")\n",
        "    test_size_input = input(\"Enter the test size as a fraction (e.g., 0.2 for 20%): \")\n",
        "    test_size = float(test_size_input)\n",
        "\n",
        "    # Run the initial pipeline steps\n",
        "    df = pipeline.load_dataset()\n",
        "    df_cleaned = pipeline.clean_data()\n",
        "    X, y = pipeline.define_target(df_cleaned, target_column_name)\n",
        "\n",
        "    # Determine the number of classes\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = pipeline.split_data(test_size=test_size)\n",
        "\n",
        "    # Define the custom sampling strategy\n",
        "    if num_classes > 2:  # Multi-class case\n",
        "        class_counts = np.bincount(y_train)\n",
        "        sampling_strategy = {i: max(class_counts) for i in range(num_classes)}\n",
        "\n",
        "        ## Can also define custom sampling strategy for each class\n",
        "        # sampling_strategy = {\n",
        "        #     0: 120,  # Resample class 0 to 120 samples\n",
        "        #     1: 100,  # Resample class 1 to 100 samples\n",
        "        #     2: 8    # Resample class 2 to 80 samples\n",
        "        # }\n",
        "\n",
        "    else:  # Binary case\n",
        "        class_counts = np.bincount(y_train)\n",
        "        sampling_strategy = {0: class_counts[0], 1: class_counts[1] * 2}  # Adjust this strategy as per need\n",
        "\n",
        "\n",
        "    # Apply custom SMOTE\n",
        "    X_smote, y_smote = apply_weighted_smote(X_train, y_train, sampling_strategy)\n",
        "\n",
        "    # Plot the distribution after SMOTE\n",
        "    pipeline.data_distribution_plot(pd.DataFrame(X_smote, columns=X_train.columns))\n",
        "    pipeline.plot_class_distribution(pd.Series(y_smote), plot_title='SMOTE Class Distribution')\n",
        "\n",
        "    # Scale the resampled datasets\n",
        "    X_smote_scaled, X_test_scaled = pipeline.apply_standard_scaler(X_smote, X_test)\n",
        "\n",
        "    # Define the directory for images\n",
        "    result_images_directory_smote = os.path.join(pipeline.results_dir, 'images_SMOTE')\n",
        "\n",
        "    # Create the directories if they don't exist\n",
        "    os.makedirs(result_images_directory_smote, exist_ok=True)\n",
        "\n",
        "    # Document to save the results\n",
        "    doc = Document()\n",
        "\n",
        "    # Train and evaluate models with the SMOTE-applied dataset\n",
        "    pipeline.train_and_evaluate_models(X_smote_scaled, X_test_scaled, y_smote, y_test, doc, result_images_directory_smote, num_classes)\n",
        "\n",
        "    # Save the results to a document\n",
        "    output_file_path = os.path.join(base_output_file_smote, 'model_evaluation_results_with_smote.docx')\n",
        "    doc.save(output_file_path)\n",
        "    print(f\"Results saved to '{output_file_path}'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    smote_pipeline()\n"
      ],
      "metadata": {
        "id": "mbPNmQVSlFdr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "615d995a-6ccb-45c3-c153-f84a801229d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created directory: /content/drive/MyDrive/Colab Notebooks/results/smote/dataset\n",
            "Enter the name of the target column: class\n",
            "Enter the test size as a fraction (e.g., 0.2 for 20%): 0.3\n",
            "Cleaned dataset saved to /content/drive/MyDrive/Colab Notebooks/results/smote/dataset/cleaned_dataset.csv\n",
            "Detected 2 unique classes in the target column.\n",
            "SMOTE Resampled X shape: (102, 41)\n",
            "SMOTE Resampled y shape: (102,)\n",
            "\n",
            "Training and evaluating Random Forest...\n",
            "Random Forest Accuracy: 0.9667\n",
            "Random Forest Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      1.00      0.97        15\n",
            "           1       1.00      0.93      0.97        15\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.97      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n",
            "\n",
            "Random Forest ROC AUC: 1.0000\n",
            "--------------------------------------------------\n",
            "\n",
            "Training and evaluating SVM...\n",
            "SVM Accuracy: 1.0000\n",
            "SVM Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        15\n",
            "           1       1.00      1.00      1.00        15\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "\n",
            "SVM ROC AUC: 1.0000\n",
            "--------------------------------------------------\n",
            "\n",
            "Training and evaluating Decision Tree...\n",
            "Decision Tree Accuracy: 0.9667\n",
            "Decision Tree Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      1.00      0.97        15\n",
            "           1       1.00      0.93      0.97        15\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.97      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n",
            "\n",
            "Decision Tree ROC AUC: 0.9667\n",
            "--------------------------------------------------\n",
            "\n",
            "Training and evaluating KNN...\n",
            "KNN Accuracy: 0.9667\n",
            "KNN Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.93      0.97        15\n",
            "           1       0.94      1.00      0.97        15\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.97      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n",
            "\n",
            "KNN ROC AUC: 1.0000\n",
            "--------------------------------------------------\n",
            "\n",
            "Training and evaluating Logistic Regression...\n",
            "Logistic Regression Accuracy: 1.0000\n",
            "Logistic Regression Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        15\n",
            "           1       1.00      1.00      1.00        15\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "\n",
            "Logistic Regression ROC AUC: 1.0000\n",
            "--------------------------------------------------\n",
            "\n",
            "Training and evaluating Keras ANN...\n",
            "Epoch 1/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.6270 - loss: 0.7091 - val_accuracy: 0.4333 - val_loss: 0.7421\n",
            "Epoch 2/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6698 - loss: 0.5720 - val_accuracy: 0.5333 - val_loss: 0.6015\n",
            "Epoch 3/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7593 - loss: 0.4665 - val_accuracy: 0.7333 - val_loss: 0.4866\n",
            "Epoch 4/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8412 - loss: 0.3831 - val_accuracy: 0.9000 - val_loss: 0.3953\n",
            "Epoch 5/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9510 - loss: 0.3159 - val_accuracy: 0.9667 - val_loss: 0.3217\n",
            "Epoch 6/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9664 - loss: 0.2605 - val_accuracy: 0.9667 - val_loss: 0.2615\n",
            "Epoch 7/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9664 - loss: 0.2144 - val_accuracy: 0.9667 - val_loss: 0.2132\n",
            "Epoch 8/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9755 - loss: 0.1766 - val_accuracy: 0.9667 - val_loss: 0.1764\n",
            "Epoch 9/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9755 - loss: 0.1462 - val_accuracy: 0.9667 - val_loss: 0.1473\n",
            "Epoch 10/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9755 - loss: 0.1220 - val_accuracy: 0.9667 - val_loss: 0.1241\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "Keras ANN Accuracy: 0.9667\n",
            "Keras ANN Precision: 1.0000\n",
            "Keras ANN Recall: 0.9333\n",
            "Keras ANN F1-Score: 0.9655\n",
            "Keras ANN ROC AUC: 1.0000\n",
            "--------------------------------------------------\n",
            "Confusion Matrix:\n",
            "[[15  0]\n",
            " [ 1 14]]\n",
            "Results saved to /content/drive/MyDrive/Colab Notebooks/results/smote/images_SMOTE/model_evaluation_results_org.docx\n",
            "Results saved to '/content/drive/MyDrive/Colab Notebooks/results/smote/model_evaluation_results_with_smote.docx'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ADASYN"
      ],
      "metadata": {
        "id": "vSbaXgleBIpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import ADASYN\n",
        "from docx import Document\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Function to apply ADASYN\n",
        "def apply_adasyn(X, y, sampling_strategy):\n",
        "    try:\n",
        "        adasyn = ADASYN(\n",
        "            sampling_strategy=sampling_strategy,  # Determines the resampling strategy. Other possible values: 'auto', float, dict, callable, default='auto'\n",
        "            random_state=42,  # Random seed for reproducibility. Other possible values: any integer, default=None\n",
        "            n_neighbors=5,  # Number of nearest neighbors to use to construct synthetic samples. Other possible values: any positive integer, default=5\n",
        "            n_jobs= None  # Number of parallel jobs to run for nearest neighbors computation. Other possible values: any integer or None, default=None\n",
        "        )\n",
        "        X_res, y_res = adasyn.fit_resample(X, y)\n",
        "        print(f\"ADASYN Resampled X shape: {X_res.shape}\")\n",
        "        print(f\"ADASYN Resampled y shape: {y_res.shape}\")\n",
        "\n",
        "        return X_res, y_res\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(f\"Error in ADASYN resampling: {e}\")\n",
        "        print(\"Falling back to the original data without resampling.\")\n",
        "        return X, y\n",
        "\n",
        "def adasyn_pipeline():\n",
        "    # Step 1: Define paths for dataset and results\n",
        "    file_path = '/content/drive/MyDrive/ResearchTask/Research_Dataset/Copy of preprocessed_merged_files.csv'  # Binary\n",
        "    # file_path = '/content/drive/MyDrive/Colab Notebooks/Original_dataset/cyberthreat.csv' # multi class\n",
        "    base_output_file = '/content/drive/MyDrive/Colab Notebooks/results/'\n",
        "    name = 'adasyn'\n",
        "\n",
        "    # Define the directory for ADASYN results\n",
        "    base_output_file_adasyn = os.path.join(base_output_file, name)\n",
        "\n",
        "    # Create the directories if they don't exist\n",
        "    os.makedirs(base_output_file_adasyn, exist_ok=True)\n",
        "\n",
        "    # Initialize the DataPipeline class\n",
        "    pipeline = DataPipeline(file_path, base_output_file_adasyn)\n",
        "\n",
        "    target_column_name = input(\"Enter the name of the target column: \")\n",
        "    test_size_input = input(\"Enter the test size as a fraction (e.g., 0.2 for 20%): \")\n",
        "    test_size = float(test_size_input)\n",
        "\n",
        "    # Run the initial pipeline steps\n",
        "    df = pipeline.load_dataset()\n",
        "    df_cleaned = pipeline.clean_data()\n",
        "    X, y = pipeline.define_target(df_cleaned, target_column_name)\n",
        "\n",
        "    # Determine the number of classes\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = pipeline.split_data(test_size=test_size)\n",
        "\n",
        "    # Define the custom sampling strategy\n",
        "    if num_classes > 2:  # Multi-class case\n",
        "        class_counts = np.bincount(y_train)\n",
        "        sampling_strategy = {i: int(max(class_counts) * 1.2) for i in range(num_classes)}\n",
        "\n",
        "        ## Can also define custom sampling strategy for each class\n",
        "        # sampling_strategy = {\n",
        "        #     0: 120,  # Resample class 0 to 120 samples\n",
        "        #     1: 100,  # Resample class 1 to 100 samples\n",
        "        #     2: 80    # Resample class 2 to 80 samples\n",
        "        # }\n",
        "\n",
        "    else:  # Binary case\n",
        "        class_counts = np.bincount(y_train)\n",
        "        sampling_strategy = {0: class_counts[0], 1: int(class_counts[1] * 1.5)}  # Adjust this strategy as per need\n",
        "\n",
        "    # Apply ADASYN with error handling\n",
        "    X_adasyn, y_adasyn = apply_adasyn(X_train, y_train, sampling_strategy)\n",
        "\n",
        "    # Plot the distribution after ADASYN\n",
        "    pipeline.data_distribution_plot(pd.DataFrame(X_adasyn, columns=X_train.columns))\n",
        "    pipeline.plot_class_distribution(pd.Series(y_adasyn), plot_title='ADASYN Class Distribution')\n",
        "\n",
        "    # Scale the resampled datasets\n",
        "    X_adasyn_scaled, X_test_scaled = pipeline.apply_standard_scaler(X_adasyn, X_test)\n",
        "\n",
        "    # Define the directory for images\n",
        "    result_images_directory_adasyn = os.path.join(pipeline.results_dir, 'images_ADASYN')\n",
        "\n",
        "    # Create the directories if they don't exist\n",
        "    os.makedirs(result_images_directory_adasyn, exist_ok=True)\n",
        "\n",
        "    # Document to save the results\n",
        "    doc = Document()\n",
        "\n",
        "    # Train and evaluate models with the ADASYN-applied dataset\n",
        "    pipeline.train_and_evaluate_models(X_adasyn_scaled, X_test_scaled, y_adasyn, y_test, doc, result_images_directory_adasyn, num_classes)\n",
        "\n",
        "    # Save the results to a document\n",
        "    output_file_path = os.path.join(base_output_file_adasyn, 'model_evaluation_results_with_adasyn.docx')\n",
        "    doc.save(output_file_path)\n",
        "    print(f\"Results saved to '{output_file_path}'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    adasyn_pipeline()\n"
      ],
      "metadata": {
        "id": "zqqS5OFwBFBZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "425a3c9a-03d5-4750-c47d-a28ed5d9b357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created directory: /content/drive/MyDrive/Colab Notebooks/results/adasyn/dataset\n",
            "Enter the name of the target column: class\n",
            "Enter the test size as a fraction (e.g., 0.2 for 20%): 0.3\n",
            "Cleaned dataset saved to /content/drive/MyDrive/Colab Notebooks/results/adasyn/dataset/cleaned_dataset.csv\n",
            "Detected 2 unique classes in the target column.\n",
            "ADASYN Resampled X shape: (86, 41)\n",
            "ADASYN Resampled y shape: (86,)\n",
            "\n",
            "Training and evaluating Random Forest...\n",
            "Random Forest Accuracy: 0.9667\n",
            "Random Forest Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      1.00      0.97        15\n",
            "           1       1.00      0.93      0.97        15\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.97      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n",
            "\n",
            "Random Forest ROC AUC: 0.9956\n",
            "--------------------------------------------------\n",
            "\n",
            "Training and evaluating SVM...\n",
            "SVM Accuracy: 1.0000\n",
            "SVM Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        15\n",
            "           1       1.00      1.00      1.00        15\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "\n",
            "SVM ROC AUC: 1.0000\n",
            "--------------------------------------------------\n",
            "\n",
            "Training and evaluating Decision Tree...\n",
            "Decision Tree Accuracy: 0.9000\n",
            "Decision Tree Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      1.00      0.91        15\n",
            "           1       1.00      0.80      0.89        15\n",
            "\n",
            "    accuracy                           0.90        30\n",
            "   macro avg       0.92      0.90      0.90        30\n",
            "weighted avg       0.92      0.90      0.90        30\n",
            "\n",
            "\n",
            "Decision Tree ROC AUC: 0.9667\n",
            "--------------------------------------------------\n",
            "\n",
            "Training and evaluating KNN...\n",
            "KNN Accuracy: 0.9667\n",
            "KNN Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.93      0.97        15\n",
            "           1       0.94      1.00      0.97        15\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.97      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n",
            "\n",
            "KNN ROC AUC: 1.0000\n",
            "--------------------------------------------------\n",
            "\n",
            "Training and evaluating Logistic Regression...\n",
            "Logistic Regression Accuracy: 1.0000\n",
            "Logistic Regression Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        15\n",
            "           1       1.00      1.00      1.00        15\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "\n",
            "Logistic Regression ROC AUC: 1.0000\n",
            "--------------------------------------------------\n",
            "\n",
            "Training and evaluating Keras ANN...\n",
            "Epoch 1/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - accuracy: 0.4182 - loss: 0.7674 - val_accuracy: 0.4333 - val_loss: 0.6889\n",
            "Epoch 2/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6186 - loss: 0.6478 - val_accuracy: 0.8333 - val_loss: 0.5738\n",
            "Epoch 3/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8269 - loss: 0.5552 - val_accuracy: 0.8667 - val_loss: 0.4850\n",
            "Epoch 4/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8989 - loss: 0.4811 - val_accuracy: 0.9333 - val_loss: 0.4169\n",
            "Epoch 5/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9417 - loss: 0.4211 - val_accuracy: 0.9667 - val_loss: 0.3631\n",
            "Epoch 6/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9689 - loss: 0.3719 - val_accuracy: 0.9667 - val_loss: 0.3225\n",
            "Epoch 7/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9747 - loss: 0.3319 - val_accuracy: 1.0000 - val_loss: 0.2889\n",
            "Epoch 8/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9845 - loss: 0.2977 - val_accuracy: 1.0000 - val_loss: 0.2599\n",
            "Epoch 9/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9845 - loss: 0.2674 - val_accuracy: 1.0000 - val_loss: 0.2342\n",
            "Epoch 10/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9845 - loss: 0.2399 - val_accuracy: 1.0000 - val_loss: 0.2108\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
            "Keras ANN Accuracy: 1.0000\n",
            "Keras ANN Precision: 1.0000\n",
            "Keras ANN Recall: 1.0000\n",
            "Keras ANN F1-Score: 1.0000\n",
            "Keras ANN ROC AUC: 1.0000\n",
            "--------------------------------------------------\n",
            "Confusion Matrix:\n",
            "[[15  0]\n",
            " [ 0 15]]\n",
            "Results saved to /content/drive/MyDrive/Colab Notebooks/results/adasyn/images_ADASYN/model_evaluation_results_org.docx\n",
            "Results saved to '/content/drive/MyDrive/Colab Notebooks/results/adasyn/model_evaluation_results_with_adasyn.docx'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensemble"
      ],
      "metadata": {
        "id": "t6RZdA3Gf0Le"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, roc_auc_score, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler, label_binarize\n",
        "from docx import Document\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def stack_models(X_train_scaled, y_train):\n",
        "    \"\"\"Define and train the Stacking Classifier.\"\"\"\n",
        "    base_learners = [\n",
        "        ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "        ('svc', SVC(probability=True, random_state=42)),\n",
        "        ('knn', KNeighborsClassifier(n_neighbors=5)),\n",
        "        ('dt', DecisionTreeClassifier(random_state=42))\n",
        "    ]\n",
        "\n",
        "    meta_model = LogisticRegression()\n",
        "    stacking_model = StackingClassifier(estimators=base_learners, final_estimator=meta_model)\n",
        "\n",
        "    stacking_model.fit(X_train_scaled, y_train)\n",
        "    return stacking_model\n",
        "\n",
        "\n",
        "def stacking_pipeline():\n",
        "    # Step 1: Define paths for dataset and results\n",
        "    file_path = '/content/drive/MyDrive/ResearchTask/Research_Dataset/Copy of preprocessed_merged_files.csv'  # Binary\n",
        "    # file_path = '/content/drive/MyDrive/Colab Notebooks/Original_dataset/cyberthreat.csv'  # Multi-class\n",
        "    base_output_file = '/content/drive/MyDrive/Colab Notebooks/results/'\n",
        "    name = 'stacking'\n",
        "\n",
        "    # Define the directory for stacking results\n",
        "    base_output_file_stacking = os.path.join(base_output_file, name)\n",
        "    os.makedirs(base_output_file_stacking, exist_ok=True)\n",
        "\n",
        "    # Initialize the DataPipeline class (assuming you have this class for data handling)\n",
        "    pipeline = DataPipeline(file_path, base_output_file_stacking)\n",
        "\n",
        "    target_column_name = input(\"Enter the name of the target column: \")\n",
        "    test_size_input = input(\"Enter the test size as a fraction (e.g., 0.2 for 20%): \")\n",
        "    test_size = float(test_size_input)\n",
        "\n",
        "    # Run the initial pipeline steps\n",
        "    df = pipeline.load_dataset()\n",
        "    df_cleaned = pipeline.clean_data()\n",
        "    X, y = pipeline.define_target(df_cleaned, target_column_name)\n",
        "\n",
        "    # Determine the number of classes\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = pipeline.split_data(test_size=test_size)\n",
        "\n",
        "    # Standardize the features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Define the directory for images\n",
        "    result_images_directory_stacking = os.path.join(pipeline.results_dir, 'images_stacking')\n",
        "    os.makedirs(result_images_directory_stacking, exist_ok=True)\n",
        "\n",
        "    # Document to save the results\n",
        "    doc = Document()\n",
        "\n",
        "    # Define models\n",
        "    models = {\n",
        "        \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "        \"SVM\": SVC(probability=True, random_state=42),\n",
        "        \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "        \"KNN\": KNeighborsClassifier(),\n",
        "        \"Logistic Regression\": LogisticRegression(),\n",
        "        \"Stacking\": stack_models(X_train_scaled, y_train),\n",
        "        \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
        "        \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "    }\n",
        "\n",
        "    # Train and evaluate models\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\nTraining and evaluating {name}...\")\n",
        "\n",
        "        # Train the model\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "        y_prob = model.predict_proba(X_test_scaled) if hasattr(model, \"predict_proba\") else None\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
        "        classification_rep = classification_report(y_test, y_pred)\n",
        "        print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"{name} Classification Report:\\n{classification_rep}\")\n",
        "\n",
        "        # ROC AUC Calculation\n",
        "        roc_auc = None\n",
        "        if num_classes > 2:  # Multi-class case\n",
        "            y_test_bin = label_binarize(y_test, classes=list(range(num_classes)))\n",
        "            if y_prob is not None:\n",
        "                try:\n",
        "                    roc_auc = roc_auc_score(y_test_bin, y_prob, multi_class='ovr')\n",
        "                except Exception as e:\n",
        "                    print(f\"Error calculating ROC AUC for {name}: {e}\")\n",
        "        elif y_prob is not None:\n",
        "            roc_auc = roc_auc_score(y_test, y_prob[:, 1])\n",
        "\n",
        "        # Print ROC AUC if available\n",
        "        if roc_auc is not None:\n",
        "            print(f\"{name} ROC AUC: {roc_auc:.4f}\")\n",
        "        else:\n",
        "            print(f\"{name} ROC AUC: Not available\")\n",
        "\n",
        "        # Plot ROC curve if applicable\n",
        "        if roc_auc is not None and y_prob is not None:\n",
        "            if num_classes == 2:\n",
        "                fpr, tpr, _ = roc_curve(y_test, y_prob[:, 1])\n",
        "            else:\n",
        "                fpr, tpr, _ = roc_curve(y_test_bin.ravel(), y_prob.ravel())\n",
        "\n",
        "            plt.figure()\n",
        "            plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "            plt.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
        "            plt.xlabel('False Positive Rate')\n",
        "            plt.ylabel('True Positive Rate')\n",
        "            plt.title(f'{name} ROC Curve')\n",
        "            plt.legend(loc=\"lower right\")\n",
        "\n",
        "            # Save the plot\n",
        "            roc_curve_path = os.path.join(result_images_directory_stacking, f\"{name}_roc_curve.png\")\n",
        "            plt.savefig(roc_curve_path)\n",
        "            plt.close()\n",
        "\n",
        "            # Add results and ROC curve image to the document\n",
        "            doc.add_heading(f\"{name} Results\", level=2)\n",
        "            doc.add_paragraph(f\"Accuracy: {accuracy:.4f}\")\n",
        "            doc.add_paragraph(\"Classification Report:\")\n",
        "            doc.add_paragraph(classification_rep)\n",
        "            doc.add_paragraph(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "            doc.add_picture(roc_curve_path, width=Inches(4.5))\n",
        "        else:\n",
        "            doc.add_heading(f\"{name} Results\", level=2)\n",
        "            doc.add_paragraph(f\"Accuracy: {accuracy:.4f}\")\n",
        "            doc.add_paragraph(\"Classification Report:\")\n",
        "            doc.add_paragraph(classification_rep)\n",
        "            doc.add_paragraph(\"ROC AUC: Not available\")\n",
        "\n",
        "    # Save the results document\n",
        "    doc_path = os.path.join(result_images_directory_stacking, \"model_evaluation_results_with_stacking.docx\")\n",
        "    doc.save(doc_path)\n",
        "    print(f\"Results saved to {doc_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    stacking_pipeline()\n"
      ],
      "metadata": {
        "id": "Pwbzcm4yJYQS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92060fa2-4d3e-43a0-f4e4-3a38832cbfc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created directory: /content/drive/MyDrive/Colab Notebooks/results/stacking/dataset\n",
            "Enter the name of the target column: class\n",
            "Enter the test size as a fraction (e.g., 0.2 for 20%): 0.2\n",
            "Cleaned dataset saved to /content/drive/MyDrive/Colab Notebooks/results/stacking/dataset/cleaned_dataset.csv\n",
            "Detected 2 unique classes in the target column.\n",
            "\n",
            "Training and evaluating Random Forest...\n",
            "Random Forest Accuracy: 0.9500\n",
            "Random Forest Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      1.00      0.95        10\n",
            "           1       1.00      0.90      0.95        10\n",
            "\n",
            "    accuracy                           0.95        20\n",
            "   macro avg       0.95      0.95      0.95        20\n",
            "weighted avg       0.95      0.95      0.95        20\n",
            "\n",
            "Random Forest ROC AUC: 1.0000\n",
            "\n",
            "Training and evaluating SVM...\n",
            "SVM Accuracy: 1.0000\n",
            "SVM Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00        10\n",
            "\n",
            "    accuracy                           1.00        20\n",
            "   macro avg       1.00      1.00      1.00        20\n",
            "weighted avg       1.00      1.00      1.00        20\n",
            "\n",
            "SVM ROC AUC: 1.0000\n",
            "\n",
            "Training and evaluating Decision Tree...\n",
            "Decision Tree Accuracy: 0.9500\n",
            "Decision Tree Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      1.00      0.95        10\n",
            "           1       1.00      0.90      0.95        10\n",
            "\n",
            "    accuracy                           0.95        20\n",
            "   macro avg       0.95      0.95      0.95        20\n",
            "weighted avg       0.95      0.95      0.95        20\n",
            "\n",
            "Decision Tree ROC AUC: 0.9500\n",
            "\n",
            "Training and evaluating KNN...\n",
            "KNN Accuracy: 0.9500\n",
            "KNN Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.90      0.95        10\n",
            "           1       0.91      1.00      0.95        10\n",
            "\n",
            "    accuracy                           0.95        20\n",
            "   macro avg       0.95      0.95      0.95        20\n",
            "weighted avg       0.95      0.95      0.95        20\n",
            "\n",
            "KNN ROC AUC: 0.9900\n",
            "\n",
            "Training and evaluating Logistic Regression...\n",
            "Logistic Regression Accuracy: 1.0000\n",
            "Logistic Regression Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00        10\n",
            "\n",
            "    accuracy                           1.00        20\n",
            "   macro avg       1.00      1.00      1.00        20\n",
            "weighted avg       1.00      1.00      1.00        20\n",
            "\n",
            "Logistic Regression ROC AUC: 1.0000\n",
            "\n",
            "Training and evaluating Stacking...\n",
            "Stacking Accuracy: 0.9500\n",
            "Stacking Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      1.00      0.95        10\n",
            "           1       1.00      0.90      0.95        10\n",
            "\n",
            "    accuracy                           0.95        20\n",
            "   macro avg       0.95      0.95      0.95        20\n",
            "weighted avg       0.95      0.95      0.95        20\n",
            "\n",
            "Stacking ROC AUC: 1.0000\n",
            "\n",
            "Training and evaluating Gradient Boosting...\n",
            "Gradient Boosting Accuracy: 0.9500\n",
            "Gradient Boosting Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      1.00      0.95        10\n",
            "           1       1.00      0.90      0.95        10\n",
            "\n",
            "    accuracy                           0.95        20\n",
            "   macro avg       0.95      0.95      0.95        20\n",
            "weighted avg       0.95      0.95      0.95        20\n",
            "\n",
            "Gradient Boosting ROC AUC: 0.9500\n",
            "\n",
            "Training and evaluating XGBoost...\n",
            "XGBoost Accuracy: 0.9000\n",
            "XGBoost Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      1.00      0.91        10\n",
            "           1       1.00      0.80      0.89        10\n",
            "\n",
            "    accuracy                           0.90        20\n",
            "   macro avg       0.92      0.90      0.90        20\n",
            "weighted avg       0.92      0.90      0.90        20\n",
            "\n",
            "XGBoost ROC AUC: 0.9000\n",
            "Results saved to /content/drive/MyDrive/Colab Notebooks/results/stacking/images_stacking/model_evaluation_results_with_stacking.docx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Step 1: Define paths for dataset and results\n",
        "    # file_path = '/content/drive/MyDrive/Colab Notebooks/Original_dataset/preprocessed_merged_files.csv' # binary class\n",
        "    file_path = '/content/drive/MyDrive/ResearchTask/Research_Dataset/Copy of preprocessed_merged_files.csv' # multi class\n",
        "    base_output_file = '/content/drive/MyDrive/ResearchTask/results/'\n",
        "    # Initialize the DataPipeline class\n",
        "    pipeline = DataPipeline(file_path, base_output_file)\n",
        "\n",
        "    target_column_name = input(\"Enter the name of the target column: \")\n",
        "    test_size_input = input(\"Enter the test size as a fraction (e.g., 0.2 for 20%): \")\n",
        "    test_size = float(test_size_input)\n",
        "\n",
        "    dataset_name = \"original dataset\"\n",
        "\n",
        "    # Run the initial pipeline up to data splitting\n",
        "    X_train_scaled, X_test_scaled, y_train, y_test = pipeline.run_pipeline(target_column_name, test_size)\n",
        "\n",
        "    display(HTML(f\"<h3 style='color: red;'>Results for {dataset_name}</h3>\"))\n",
        "\n",
        "    # Define the directory for images\n",
        "    result_images_directory = os.path.join(pipeline.results_dir, 'images')\n",
        "\n",
        "    # Create the directory if it doesn't exist\n",
        "    os.makedirs(result_images_directory, exist_ok=True)\n",
        "\n",
        "    # Document to save the results\n",
        "    doc = Document()\n",
        "\n",
        "    # Train and evaluate the models\n",
        "    pipeline.train_and_evaluate_models(X_train_scaled, X_test_scaled, y_train, y_test, doc, result_images_directory, pipeline.num_classes)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XoGVCINUfzUg",
        "outputId": "5e39c91b-4086-426e-e14c-7255133daa3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the name of the target column: class\n",
            "Enter the test size as a fraction (e.g., 0.2 for 20%): 0.2\n",
            "Dataset summary saved to /content/drive/MyDrive/ResearchTask/results/dataset/dataset_summary.csv\n",
            "Cleaned dataset saved to /content/drive/MyDrive/ResearchTask/results/dataset/cleaned_dataset.csv\n",
            "Detected 2 unique classes in the target column.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h3 style='color: red;'>Results for original dataset</h3>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training and evaluating Random Forest...\n",
            "Random Forest Accuracy: 0.9500\n",
            "Random Forest Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      1.00      0.95        10\n",
            "           1       1.00      0.90      0.95        10\n",
            "\n",
            "    accuracy                           0.95        20\n",
            "   macro avg       0.95      0.95      0.95        20\n",
            "weighted avg       0.95      0.95      0.95        20\n",
            "\n",
            "\n",
            "Random Forest ROC AUC: 0.9900\n",
            "--------------------------------------------------\n",
            "\n",
            "Training and evaluating SVM...\n",
            "SVM Accuracy: 1.0000\n",
            "SVM Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00        10\n",
            "\n",
            "    accuracy                           1.00        20\n",
            "   macro avg       1.00      1.00      1.00        20\n",
            "weighted avg       1.00      1.00      1.00        20\n",
            "\n",
            "\n",
            "SVM ROC AUC: 1.0000\n",
            "--------------------------------------------------\n",
            "\n",
            "Training and evaluating Decision Tree...\n",
            "Decision Tree Accuracy: 0.9500\n",
            "Decision Tree Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      1.00      0.95        10\n",
            "           1       1.00      0.90      0.95        10\n",
            "\n",
            "    accuracy                           0.95        20\n",
            "   macro avg       0.95      0.95      0.95        20\n",
            "weighted avg       0.95      0.95      0.95        20\n",
            "\n",
            "\n",
            "Decision Tree ROC AUC: 0.9500\n",
            "--------------------------------------------------\n",
            "\n",
            "Training and evaluating KNN...\n",
            "KNN Accuracy: 0.9500\n",
            "KNN Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.90      0.95        10\n",
            "           1       0.91      1.00      0.95        10\n",
            "\n",
            "    accuracy                           0.95        20\n",
            "   macro avg       0.95      0.95      0.95        20\n",
            "weighted avg       0.95      0.95      0.95        20\n",
            "\n",
            "\n",
            "KNN ROC AUC: 0.9900\n",
            "--------------------------------------------------\n",
            "\n",
            "Training and evaluating Logistic Regression...\n",
            "Logistic Regression Accuracy: 1.0000\n",
            "Logistic Regression Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00        10\n",
            "\n",
            "    accuracy                           1.00        20\n",
            "   macro avg       1.00      1.00      1.00        20\n",
            "weighted avg       1.00      1.00      1.00        20\n",
            "\n",
            "\n",
            "Logistic Regression ROC AUC: 1.0000\n",
            "--------------------------------------------------\n",
            "\n",
            "Training and evaluating Keras ANN...\n",
            "Epoch 1/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step - accuracy: 0.3651 - loss: 0.7935 - val_accuracy: 0.4500 - val_loss: 0.6859\n",
            "Epoch 2/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5324 - loss: 0.6711 - val_accuracy: 0.8000 - val_loss: 0.5815\n",
            "Epoch 3/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8884 - loss: 0.5778 - val_accuracy: 0.8500 - val_loss: 0.4990\n",
            "Epoch 4/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9244 - loss: 0.5028 - val_accuracy: 0.8500 - val_loss: 0.4337\n",
            "Epoch 5/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9362 - loss: 0.4413 - val_accuracy: 0.9500 - val_loss: 0.3806\n",
            "Epoch 6/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9449 - loss: 0.3901 - val_accuracy: 0.9500 - val_loss: 0.3376\n",
            "Epoch 7/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9552 - loss: 0.3471 - val_accuracy: 0.9500 - val_loss: 0.3023\n",
            "Epoch 8/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9552 - loss: 0.3106 - val_accuracy: 0.9500 - val_loss: 0.2722\n",
            "Epoch 9/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9552 - loss: 0.2788 - val_accuracy: 0.9500 - val_loss: 0.2457\n",
            "Epoch 10/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9552 - loss: 0.2504 - val_accuracy: 0.9500 - val_loss: 0.2215\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "Keras ANN Accuracy: 0.9500\n",
            "Keras ANN Precision: 1.0000\n",
            "Keras ANN Recall: 0.9000\n",
            "Keras ANN F1-Score: 0.9474\n",
            "Keras ANN ROC AUC: 1.0000\n",
            "--------------------------------------------------\n",
            "Confusion Matrix:\n",
            "[[10  0]\n",
            " [ 1  9]]\n",
            "Results saved to /content/drive/MyDrive/ResearchTask/results/images/model_evaluation_results_org.docx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Reuhp30dqb9O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}